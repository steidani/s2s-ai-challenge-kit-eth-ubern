{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML model to correct predictions of week 3-4 & 5-6\n",
    "\n",
    "This notebook create a Machine Learning `ML_model` to predict weeks 3-4 & 5-6 based on `S2S` weeks 3-4 & 5-6 forecasts and is compared to `CPC` observations for the [`s2s-ai-challenge`](https://s2s-ai-challenge.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: `ML-based mean bias reduction`\n",
    "\n",
    "- calculate the ML-based bias from 2000-2019 deterministic ensemble mean forecast\n",
    "- remove that the ML-based bias from 2020 forecast deterministic ensemble mean forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used\n",
    "\n",
    "type: renku datasets\n",
    "\n",
    "Training-input for Machine Learning model:\n",
    "- hindcasts of models:\n",
    "    - ECMWF: `ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr`\n",
    "\n",
    "Forecast-input for Machine Learning model:\n",
    "- real-time 2020 forecasts of models:\n",
    "    - ECMWF: `ecmwf_forecast-input_2020_biweekly_deterministic.zarr`\n",
    "\n",
    "Compare Machine Learning model forecast against against ground truth:\n",
    "- `CPC` observations:\n",
    "    - `hindcast-like-observations_biweekly_deterministic.zarr`\n",
    "    - `forecast-like-observations_2020_biweekly_deterministic.zarr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources used\n",
    "for training, details in reproducibility\n",
    "\n",
    "- platform: renku\n",
    "- memory: 8 GB\n",
    "- processors: 2 CPU\n",
    "- storage required: 10 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safeguards\n",
    "\n",
    "All points have to be [x] checked. If not, your submission is invalid.\n",
    "\n",
    "Changes to the code after submissions are not possible, as the `commit` before the `tag` will be reviewed.\n",
    "(Only in exceptions and if previous effort in reproducibility can be found, it may be allowed to improve readability and reproducibility after November 1st 2021.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safeguards to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting?wprov=sfti1) \n",
    "\n",
    "If the organizers suspect overfitting, your contribution can be disqualified.\n",
    "\n",
    "  - [x] We did not use 2020 observations in training (explicit overfitting and cheating)\n",
    "  - [x] We did not repeatedly verify my model on 2020 observations and incrementally improved my RPSS (implicit overfitting)\n",
    "  - [x] We provide RPSS scores for the training period with script `print_RPS_per_year`, see in section 6.3 `predict`.\n",
    "  - [x] We tried our best to prevent [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)?wprov=sfti1).\n",
    "  - [x] We honor the `train-validate-test` [split principle](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets). This means that the hindcast data is split into `train` and `validate`, whereas `test` is withheld.\n",
    "  - [x] We did not use `test` explicitly in training or implicitly in incrementally adjusting parameters.\n",
    "  - [x] We considered [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safeguards for Reproducibility\n",
    "Notebook/code must be independently reproducible from scratch by the organizers (after the competition), if not possible: no prize\n",
    "  - [x] All training data is publicly available (no pre-trained private neural networks, as they are not reproducible for us)\n",
    "  - [x] Code is well documented, readable and reproducible.\n",
    "  - [x] Code to reproduce training and predictions is preferred to run within a day on the described architecture. If the training takes longer than a day, please justify why this is needed. Please do not submit training piplelines, which take weeks to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos to improve template\n",
    "\n",
    "This is just a demo.\n",
    "\n",
    "- [ ] use multiple predictor variables and two predicted variables\n",
    "- [ ] for both `lead_time`s in one go\n",
    "- [ ] consider seasonality, for now all `forecast_time` months are mixed\n",
    "- [ ] make probabilistic predictions with `category` dim, for now works deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of this notebook\n",
    "\n",
    "* makes probabilistic predictions for categories\n",
    "* only for one lead time and temperature variable\n",
    "* based on ANN with ensemble spread (max - min) and ensemble mean as input (with removed annual cycle), uses softmax to return class probabilities\n",
    "* low skill (accuracy ~ 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "xr.set_options(display_style='text')\n",
    "\n",
    "\n",
    "\n",
    "from dask.utils import format_bytes\n",
    "import xskillscore as xs\n",
    "\n",
    "%matplotlib inline \n",
    "#so that figures appear again\n",
    "\n",
    "#for prediction\n",
    "from scripts import make_probabilistic\n",
    "from scripts import add_valid_time_from_forecast_reference_time_and_lead_time\n",
    "from scripts import skill_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../template/data' #if you change this you also have to adjust the git lfs pull paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training data\n",
    "\n",
    "preprocessing of input data may be done in separate notebook/script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindcast\n",
    "\n",
    "get weekly initialized hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed as renku dataset\n",
    "!git lfs pull ../template/data/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = xr.open_zarr(f'{cache_path}/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "corresponding to hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed as renku dataset\n",
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019 = xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr', consolidated=True)#[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_terciled.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019_terciled = xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_terciled.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select region\n",
    "\n",
    "to make life easier for the beginning --> no periodic padding needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = hind_2000_2019.sel(longitude = slice(0,30), latitude = slice(70,40))\n",
    "obs_2000_2019 = obs_2000_2019.sel(longitude = slice(0,30), latitude = slice(70,40))\n",
    "obs_2000_2019_terciled = obs_2000_2019_terciled.sel(longitude = slice(0,30), latitude = slice(70,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hind_2000_2019\n",
    "#obs_2000_2019_terciled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time is the forecast_time\n",
    "time_train_start,time_train_end='2000','2017' # train\n",
    "time_valid_start,time_valid_end='2018','2019' # valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weatherbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on [Weatherbench](https://github.com/pangeo-data/WeatherBench/blob/master/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once only and dont commit\n",
    "#!git clone https://github.com/pangeo-data/WeatherBench/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'WeatherBench')\n",
    "from WeatherBench.src.train_nn import PeriodicConv2D, create_predictions#DataGenerator, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v='t2m'\n",
    "bs=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://s2s-ai-challenge.github.io/\n",
    "\n",
    "We deal with two fundamentally different variables here: \n",
    "- Total precipitation is precipitation flux pr accumulated over lead_time until valid_time and therefore describes a point observation. \n",
    "- 2m temperature is averaged over lead_time(valid_time) and therefore describes an average observation. \n",
    "\n",
    "The submission file data model unifies both approaches and assigns 14 days for week 3-4 and 28 days for week 5-6 marking the first day of the biweekly aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 bi-weekly `lead_time`: week 3-4\n",
    "lead = hind_2000_2019.lead_time[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train:\n",
    "#uses only ensemble mean so far\n",
    "fct_train = hind_2000_2019.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "verif_train = obs_2000_2019_terciled.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "\n",
    "#make sure that all nans from obs are also nan in hind, if hind contains nans, this does not work!\n",
    "fct_train = fct_train.where(verif_train.mean('category', skipna = False).notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "fct_valid = hind_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "verif_valid = obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "#make sure that all nans from obs are also nan in hind, if hind contains nans, this does not work!\n",
    "fct_valid = fct_valid.where(verif_valid.mean('category', skipna = False).notnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bias from fct\n",
    "did not improve the skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_train = obs_2000_2019.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "obs_valid = obs_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mean_bias_reduction\n",
    "from scripts import add_year_week_coords\n",
    "fct_train_bias = add_year_week_coords(fct_train.mean('realization') - obs_train).groupby('week').mean().compute()\n",
    "fct_valid_bias = add_year_week_coords(fct_valid.mean('realization') - obs_valid).groupby('week').mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct_train = add_year_week_coords(fct_train) - fct_train_bias.sel(week=fct_train.week)\n",
    "fct_valid = add_year_week_coords(fct_valid) - fct_valid_bias.sel(week=fct_valid.week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import add_year_week_coords\n",
    "def ann_preprocess(ds, v,lead):\n",
    "    ds = ds.sel(lead_time = lead)\n",
    "    \n",
    "    #remove annual cycle for each location \n",
    "    ds = add_year_week_coords(ds)\n",
    "    ens_mean = ds.mean('realization')\n",
    "    ens_mean = ens_mean - ens_mean.groupby('week').mean(['forecast_time'])\n",
    "\n",
    "    #compute ensemble spread, remove local seasonal cycle\n",
    "    spread = ds.max('realization')-ds.min('realization')\n",
    "    spread = spread/spread.groupby('week').mean(['forecast_time'])\n",
    "    \n",
    "    #combine data arrays\n",
    "    spread = spread.to_dataset(name = 'spread_{}'.format(v))\n",
    "    ens_mean = ens_mean.to_dataset(name = 'mean_{}'.format(v))\n",
    "    combined = xr.combine_by_coords([ens_mean, spread])\n",
    "    combined= combined.sel({'week' : ds.coords['week']})\n",
    "    \n",
    "    df = combined.to_dataframe()\n",
    "    df = df.drop(['lead_time','valid_time','week','year'], axis =1).reset_index()\n",
    "    \n",
    "    df = df.dropna(axis = 0)\n",
    "    \n",
    "    #to get input shape back later\n",
    "    df_ref = df\n",
    "    \n",
    "    df = df.drop(['forecast_time','latitude','longitude'], axis = 1)\n",
    "    df = (df - df.mean(axis = 0))/df.std(axis = 0)#standardize everything\n",
    "    \n",
    "    return df, df_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_preprocess_label(ds,v,lead):\n",
    "    df = ds.sel(lead_time = lead).to_dataframe()\n",
    "    df = df.drop(['lead_time','valid_time'], axis =1).reset_index()\n",
    "    df = df.pivot(index = ['forecast_time', 'latitude','longitude'], columns = 'category', values = v).reset_index()\n",
    "    df.rename_axis(None, inplace = True, axis = 1)\n",
    "    df = df.dropna(axis = 0)\n",
    "    \n",
    "    \n",
    "    df=df.drop(['forecast_time','latitude','longitude'], axis = 1)\n",
    "    df=df[['below normal', 'near normal','above normal']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataframes\n",
    "\n",
    "df_verif_train = ann_preprocess_label(verif_train, v, lead)\n",
    "df_fct_train, df_fct_train_ref = ann_preprocess(fct_train, v, lead)\n",
    "df_verif_valid = ann_preprocess_label(verif_valid, v, lead)\n",
    "df_fct_valid, df_fct_valid_ref = ann_preprocess(fct_valid, v, lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verif_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fct_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "\n",
    "ann = keras.models.Sequential([\n",
    "    Dense(10, input_shape=(2,), activation='relu'),\n",
    "    #Dropout(0.2),\n",
    "    Dense(3),\n",
    "    Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')#keras.optimizers.Adam(1e-4), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.fit(df_fct_train, df_verif_train, batch_size = 100, epochs=2, validation_data=(df_fct_valid, df_verif_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###predict plus first postprocessing\n",
    "\n",
    "predicted_bins = pd.DataFrame(ann.predict(df_fct_valid), columns = df_verif_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_output(output, df_ref,v):\n",
    "    #add columns\n",
    "    output = output.assign(latitude = df_ref.latitude, longitude = df_ref.longitude,\n",
    "                           forecast_time = df_ref.forecast_time)\n",
    "    #merge category columns into one\n",
    "    output = output.melt(id_vars = ['forecast_time','latitude','longitude'], var_name = 'category', \n",
    "                                       value_name = v)#'t2m'\n",
    "    #create MultiIndex\n",
    "    output = output.pivot_table(values = v, index = ['latitude','longitude','forecast_time','category'])\n",
    "    \n",
    "    #convert to dataset\n",
    "    xr_output = xr.Dataset.from_dataframe(output)\n",
    "    \n",
    "    return xr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_predicted_bins = postprocess_output(predicted_bins, df_fct_valid_ref, v=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change order of categories\n",
    "xr_predicted_bins = xr_predicted_bins.reindex(category=[xr_predicted_bins.category[1].values, \n",
    "                                                                xr_predicted_bins.category[2].values, \n",
    "                                                                xr_predicted_bins.category[0].values ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_predicted_bins.isel(forecast_time = 0)['t2m'].plot(col = 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_valid.isel(forecast_time = 0, lead_time = 0).to_dataset()['t2m'].plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using obs as input without the spread feature improved the model performance (to 0.4)\n",
    "this is still really low, so "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc\n",
    "tercile_edges = xr.open_dataset(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tercile_edges = tercile_edges.sel(longitude = slice(0,30), latitude = slice(70,40))\n",
    "tercile_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import make_probabilistic\n",
    "\n",
    "obs_preds = make_probabilistic(obs_valid, tercile_edges)\n",
    "obs_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_preds.isel(forecast_time = 0, lead_time = 0)['t2m'].plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to create tercile_edges\n",
    "add week for groupby, see https://renkulab.io/gitlab/aaron.spring/s2s-ai-challenge/-/issues/29\n",
    "\n",
    "obs_2000_2019 = add_year_week_coords(obs_2000_2019)\n",
    "obs_2000_2019.chunk({'forecast_time':-1,'longitude':'auto'}).groupby('week').quantile(q=[1./3.,2./3.], \n",
    "               dim='forecast_time').rename({'quantile':'category_edge'}).astype('float32').to_netcdf(tercile_file)\n",
    "               \n",
    "tercile edges und weeks beziehen sich auf forecast date. i.e. die terciles für week 1 mit lead_time = 3-4wochen beziehen sich auf die Klimatologie für valid_time ende januar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://phoenixnap.com/kb/linux-commands-check-memory-usage\n",
    "!free -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
