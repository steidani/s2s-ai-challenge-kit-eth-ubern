{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML model to correct predictions of week 3-4 & 5-6\n",
    "\n",
    "This notebook create a Machine Learning `ML_model` to predict weeks 3-4 & 5-6 based on `S2S` weeks 3-4 & 5-6 forecasts and is compared to `CPC` observations for the [`s2s-ai-challenge`](https://s2s-ai-challenge.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: `ML-based mean bias reduction`\n",
    "\n",
    "- calculate the ML-based bias from 2000-2019 deterministic ensemble mean forecast\n",
    "- remove that the ML-based bias from 2020 forecast deterministic ensemble mean forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used\n",
    "\n",
    "type: renku datasets\n",
    "\n",
    "Training-input for Machine Learning model:\n",
    "- hindcasts of models:\n",
    "    - ECMWF: `ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr`\n",
    "\n",
    "Forecast-input for Machine Learning model:\n",
    "- real-time 2020 forecasts of models:\n",
    "    - ECMWF: `ecmwf_forecast-input_2020_biweekly_deterministic.zarr`\n",
    "\n",
    "Compare Machine Learning model forecast against against ground truth:\n",
    "- `CPC` observations:\n",
    "    - `hindcast-like-observations_biweekly_deterministic.zarr`\n",
    "    - `forecast-like-observations_2020_biweekly_deterministic.zarr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources used\n",
    "for training, details in reproducibility\n",
    "\n",
    "- platform: renku\n",
    "- memory: 8 GB\n",
    "- processors: 2 CPU\n",
    "- storage required: 10 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safeguards\n",
    "\n",
    "All points have to be [x] checked. If not, your submission is invalid.\n",
    "\n",
    "Changes to the code after submissions are not possible, as the `commit` before the `tag` will be reviewed.\n",
    "(Only in exceptions and if previous effort in reproducibility can be found, it may be allowed to improve readability and reproducibility after November 1st 2021.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safeguards to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting?wprov=sfti1) \n",
    "\n",
    "If the organizers suspect overfitting, your contribution can be disqualified.\n",
    "\n",
    "  - [x] We did not use 2020 observations in training (explicit overfitting and cheating)\n",
    "  - [x] We did not repeatedly verify my model on 2020 observations and incrementally improved my RPSS (implicit overfitting)\n",
    "  - [x] We provide RPSS scores for the training period with script `print_RPS_per_year`, see in section 6.3 `predict`.\n",
    "  - [x] We tried our best to prevent [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)?wprov=sfti1).\n",
    "  - [x] We honor the `train-validate-test` [split principle](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets). This means that the hindcast data is split into `train` and `validate`, whereas `test` is withheld.\n",
    "  - [x] We did not use `test` explicitly in training or implicitly in incrementally adjusting parameters.\n",
    "  - [x] We considered [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safeguards for Reproducibility\n",
    "Notebook/code must be independently reproducible from scratch by the organizers (after the competition), if not possible: no prize\n",
    "  - [x] All training data is publicly available (no pre-trained private neural networks, as they are not reproducible for us)\n",
    "  - [x] Code is well documented, readable and reproducible.\n",
    "  - [x] Code to reproduce training and predictions is preferred to run within a day on the described architecture. If the training takes longer than a day, please justify why this is needed. Please do not submit training piplelines, which take weeks to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos to improve template\n",
    "\n",
    "This is just a demo.\n",
    "\n",
    "- [ ] use multiple predictor variables and two predicted variables\n",
    "- [ ] for both `lead_time`s in one go\n",
    "- [ ] consider seasonality, for now all `forecast_time` months are mixed\n",
    "- [ ] make probabilistic predictions with `category` dim, for now works deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of this notebook\n",
    "\n",
    "* makes probabilistic predictions for categories\n",
    "* only for one lead time and temperature variable\n",
    "* based on ANN with ensemble spread (std), climatological spread, ensemble mean(with removed annual cycle), lat/lon, week as input , uses softmax to return class probabilities\n",
    "* always use train data to remove annual cycle and to compute other summary statistics\n",
    "* masking to have the same NANs over the whole period\n",
    "* L2 regularization and early stopping\n",
    "* skill still low (accuracy ~ 0.37)\n",
    "* categories are unequally distributed in the observations (near-normal is less frequent)\n",
    "* large differences between obs and fct (each with removed annual cycle)\n",
    "* improved standardization (outside of pre-processing, always using train data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "xr.set_options(display_style='text')\n",
    "\n",
    "\n",
    "\n",
    "from dask.utils import format_bytes\n",
    "import xskillscore as xs\n",
    "\n",
    "%matplotlib inline \n",
    "#so that figures appear again\n",
    "\n",
    "#for prediction\n",
    "from scripts import make_probabilistic\n",
    "from scripts import add_valid_time_from_forecast_reference_time_and_lead_time\n",
    "from scripts import skill_by_year\n",
    "from scripts import add_year_week_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../template/data' #if you change this you also have to adjust the git lfs pull paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training data\n",
    "\n",
    "preprocessing of input data may be done in separate notebook/script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindcast\n",
    "\n",
    "get weekly initialized hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed as renku dataset\n",
    "!git lfs pull ../template/data/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = xr.open_zarr(f'{cache_path}/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "corresponding to hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed as renku dataset\n",
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019 = xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr', consolidated=True)#[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_terciled.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019_terciled = xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_terciled.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select region\n",
    "\n",
    "to make life easier for the beginning --> no periodic padding needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = slice(90,0)\n",
    "lon = slice(0,90) #negative,positive will not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = hind_2000_2019.sel(longitude = lon, latitude = lat)\n",
    "obs_2000_2019 = obs_2000_2019.sel(longitude = lon, latitude = lat)\n",
    "obs_2000_2019_terciled = obs_2000_2019_terciled.sel(longitude = lon, latitude = lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time is the forecast_time\n",
    "time_train_start,time_train_end='2000','2017' # train\n",
    "time_valid_start,time_valid_end='2018','2019' # valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weatherbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on [Weatherbench](https://github.com/pangeo-data/WeatherBench/blob/master/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once only and dont commit\n",
    "#!git clone https://github.com/pangeo-data/WeatherBench/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'WeatherBench')\n",
    "from WeatherBench.src.train_nn import PeriodicConv2D, create_predictions#DataGenerator, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v='t2m'\n",
    "bs=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://s2s-ai-challenge.github.io/\n",
    "\n",
    "We deal with two fundamentally different variables here: \n",
    "- Total precipitation is precipitation flux pr accumulated over lead_time until valid_time and therefore describes a point observation. \n",
    "- 2m temperature is averaged over lead_time(valid_time) and therefore describes an average observation. \n",
    "\n",
    "The submission file data model unifies both approaches and assigns 14 days for week 3-4 and 28 days for week 5-6 marking the first day of the biweekly aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 bi-weekly `lead_time`: week 3-4\n",
    "lead = hind_2000_2019.lead_time[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask: same missing values at all forecast_times\n",
    "mask = xr.where(obs_2000_2019.notnull(),1,np.nan).mean('forecast_time', skipna = False)\n",
    "mask\n",
    "\n",
    "#What if hind contains nan?--> precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train:\n",
    "#uses only ensemble mean so far\n",
    "fct_train = hind_2000_2019.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "verif_train = obs_2000_2019_terciled.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "\n",
    "fct_train = fct_train.where(mask[v].notnull())\n",
    "verif_train = verif_train.where(mask[v].notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "fct_valid = hind_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "verif_valid = obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "\n",
    "fct_valid = fct_valid.where(mask[v].notnull())\n",
    "verif_valid = verif_valid.where(mask[v].notnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bias from fct\n",
    "did not improve the skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs_train = obs_2000_2019.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "obs_valid = obs_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#from mean_bias_reduction\n",
    "from scripts import add_year_week_coords\n",
    "fct_train_bias = add_year_week_coords(fct_train.mean('realization') - obs_train).groupby('week').mean().compute()\n",
    "fct_valid_bias = add_year_week_coords(fct_valid.mean('realization') - obs_valid).groupby('week').mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fct_train = add_year_week_coords(fct_train) - fct_train_bias.sel(week=fct_train.week)\n",
    "fct_valid = add_year_week_coords(fct_valid) - fct_valid_bias.sel(week=fct_valid.week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_annualcycle(ds, ds_train):\n",
    "    #remove annual cycle for each location \n",
    "    \n",
    "    ds = add_year_week_coords(ds)\n",
    "    ds_train = add_year_week_coords(ds_train)\n",
    "    \n",
    "    if 'realization' in ds_train.coords:#always use train data to compute the annual cycle\n",
    "        ens_mean = ds_train.mean('realization')\n",
    "    else:\n",
    "        ens_mean = ds_train\n",
    "\n",
    "    ds_stand = ds - ens_mean.groupby('week').mean(['forecast_time'])\n",
    "\n",
    "    ds_stand = ds_stand.sel({'week' : ds.coords['week']})\n",
    "    ds_stand = ds_stand.drop(['week','year'])\n",
    "    ds_stand\n",
    "    return ds_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_preprocess(ds, ds_train, v,lead):\n",
    "\n",
    "    ds = ds.sel(lead_time = lead)\n",
    "    ds_train = ds_train.sel(lead_time = lead)\n",
    "\n",
    "    #remove annual cycle for each location \n",
    "    ds = rm_annualcycle(ds, ds_train)\n",
    "\n",
    "    #compute ensemble mean\n",
    "    ens_mean = ds.mean('realization')\n",
    "\n",
    "    #compute ensemble spread\n",
    "    ens_spread = ds.std('realization')\n",
    "\n",
    "    #provide climatological ensemble spread #from obs or from ens?\n",
    "    ds_train_weekly = add_year_week_coords(ds_train)\n",
    "    if 'realization' in ds_train_weekly.coords:\n",
    "        ds_train_weekly = ds_train_weekly#.mean('realization')\n",
    "\n",
    "    clim_spread = ds_train_weekly.groupby('week').std(['forecast_time','realization'])\n",
    "    clim_spread = clim_spread.sel({'week' : add_year_week_coords(ds).coords['week']})\n",
    "    clim_spread = clim_spread.drop(['week','year'])\n",
    "    \n",
    "    #provide time feature\n",
    "    week = add_year_week_coords(ds)\n",
    "    week_ = np.cos(2*np.pi/53*(week.week +53/2))\n",
    "    week_ = week_.drop(['week','year'])\n",
    "    week_ = week_.expand_dims({'longitude': clim_spread.longitude, 'latitude': clim_spread.latitude})\n",
    "    week_ = week_.transpose('forecast_time', 'latitude', 'longitude')\n",
    "    \n",
    "    #combine data arrays\n",
    "    ens_mean = ens_mean.to_dataset(name = 'mean_{}'.format(v))\n",
    "    spread = ens_spread.to_dataset(name = 'spread_{}'.format(v))\n",
    "    clim_spread = clim_spread.to_dataset(name = 'clim_spread_{}'.format(v))\n",
    "    week_ = week_.to_dataset(name = 'week')\n",
    "    combined = xr.combine_by_coords([ens_mean, spread, clim_spread, week_])\n",
    "\n",
    "    df = combined.to_dataframe()\n",
    "    df = df.drop(['lead_time','valid_time'], axis =1).reset_index()\n",
    "    \n",
    "    df = df.dropna(axis = 0)\n",
    "    \n",
    "    #to get input shape back later\n",
    "    df_ref = df\n",
    "    \n",
    "    df = df.drop(['forecast_time'], axis = 1)#,'latitude','longitude'\n",
    "    \n",
    "    return df, df_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_preprocess_label(ds,v,lead):\n",
    "    df = ds.sel(lead_time = lead).to_dataframe()\n",
    "    df = df.drop(['lead_time','valid_time'], axis =1).reset_index()\n",
    "    df = df.pivot(index = ['forecast_time', 'latitude','longitude'], columns = 'category', values = v).reset_index()\n",
    "    df.rename_axis(None, inplace = True, axis = 1)\n",
    "    df = df.dropna(axis = 0)\n",
    "    \n",
    "    \n",
    "    df=df.drop(['forecast_time','latitude','longitude'], axis = 1)\n",
    "    df=df[['below normal', 'near normal','above normal']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataframes\n",
    "\n",
    "df_verif_train = ann_preprocess_label(verif_train, v, lead)\n",
    "df_fct_train, df_fct_train_ref = ann_preprocess(fct_train, fct_train, v, lead)\n",
    "df_verif_valid = ann_preprocess_label(verif_valid, v, lead)\n",
    "df_fct_valid, df_fct_valid_ref = ann_preprocess(fct_valid, fct_train, v, lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize input\n",
    "\n",
    "mean_fct_train = df_fct_train.mean(axis = 0)\n",
    "std_fct_train = df_fct_train.std(axis = 0)\n",
    "\n",
    "#validation set using train mean and std\n",
    "df_fct_valid   = (df_fct_valid - mean_fct_train)/std_fct_train\n",
    "\n",
    "df_fct_train   = (df_fct_train - mean_fct_train)/std_fct_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verif_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fct_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fct_train_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "\n",
    "ann = keras.models.Sequential([\n",
    "    Dense(10, input_shape=(6,), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)), #activation='relu'),\n",
    "    Activation('elu'),\n",
    "    #Dense(10, input_shape=(10,), kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)), #activation='relu'),\n",
    "    #Activation('elu'),\n",
    "    #Dropout(0.4),\n",
    "    Dense(3),# activation='softmax'),\n",
    "    #x = x+ log(1/3) ###add climatological probabilities\n",
    "    Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')#keras.optimizers.Adam(0.05))#keras.optimizers.Adam(1e-4), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping\n",
    "import tensorflow as tf\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.fit(df_fct_train, df_verif_train, batch_size = 100, epochs=5, validation_data=(df_fct_valid, df_verif_valid),\n",
    "        callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###predict plus first postprocessing\n",
    "\n",
    "predicted_bins = pd.DataFrame(ann.predict(df_fct_valid), columns = df_verif_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_output(output, df_ref, ds_input, v):\n",
    "    #add columns\n",
    "    \n",
    "    output['latitude'] = df_ref.latitude.values\n",
    "    output['longitude'] = df_ref.longitude.values\n",
    "    output['forecast_time'] = df_ref.forecast_time.values\n",
    "    \n",
    "    #merge category columns into one\n",
    "    output = output.melt(id_vars = ['forecast_time','latitude','longitude'], var_name = 'category', \n",
    "                                       value_name = v)#'t2m'\n",
    "    \n",
    "    #create MultiIndex\n",
    "    output = output.pivot_table(values = v, index = ['latitude','longitude','forecast_time','category'])\n",
    "    \n",
    "    #convert to dataset\n",
    "    xr_output = xr.Dataset.from_dataframe(output)\n",
    "    \n",
    "    #retain the complete coords    \n",
    "    temp = ds_input.sel(lead_time = lead).drop(['valid_time','lead_time'])\n",
    "    temp = temp.to_dataset(name = 'zeros')\n",
    "    merged = xr.merge([xr_output, temp])\n",
    "    merged = merged.drop('zeros')\n",
    "\n",
    "    return merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_predicted_bins = postprocess_output(predicted_bins, df_fct_valid_ref, fct_valid, v=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change order of categories\n",
    "xr_predicted_bins = xr_predicted_bins.reindex(category=[xr_predicted_bins.category[1].values, \n",
    "                                                                xr_predicted_bins.category[2].values, \n",
    "                                                                xr_predicted_bins.category[0].values ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction does not depend on the forecast_time!!!\n",
    "\n",
    "if lat/lon is used as features, predicted fields become even smoother and the RPSS is marginally lower\n",
    "\n",
    "week seems to slightly improve the forecast (in terms of accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_predicted_bins['t2m'].isel(forecast_time = 0).plot(col = 'category')\n",
    "#xr_predicted_bins['t2m'].isel(forecast_time = slice(0,50,10)).plot(col = 'category', row = 'forecast_time')#.isel(forecast_time = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground truth\n",
    "verif_valid.isel(forecast_time = 0, lead_time = 0).to_dataset()['t2m'].plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climatological category probabilities for the observations in the train set (i.e. train labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_cycle = add_year_week_coords(verif_train)\n",
    "annual_cycle = annual_cycle.groupby('week').mean(['forecast_time'])\n",
    "#annual_cycle.sel({'week' : verif_train.coords['week']})\n",
    "annual_cycle.sel(week = 1, lead_time = lead).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for week 1, the category 'near normal' is less frequent than the other two categories.\n",
    "\n",
    "This holds also for most locations in all the other weeks (see next figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for obs, only 18 years... 5/18 = 0.27 6/18 =0.33 / 7/18 = 0.38\n",
    "annual_cycle = annual_cycle.sel(lead_time = lead).stack( z = ('latitude','longitude')).reset_index(\"z\")\n",
    "annual_cycle.plot.line(hue = 'z', col = 'category', add_legend = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_cycle.sum('z').plot.line(hue = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summed up over all grid points, it is obvious, that the **near-normal category is underrepresented**. There is no indication of seasonal differences in the climatological category probabilities. The underrepresentation is most likely  the result of using the ensemble forecasts for computing the category edges. The ensemble forecasts are most likely underdispersive. Therefore, it actually \"makes sense\" that the ANN predicts lower probabilities for the near normal category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study for 2018-01-02 to compare the anomaly fields (removed annual cycle) of the forecasts to the observations\n",
    "\n",
    "potential skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = fct_valid\n",
    "ds_train = fct_train\n",
    "ds = ds.sel(lead_time = lead)\n",
    "ds_train = ds_train.sel(lead_time = lead)\n",
    "\n",
    "#remove annual cycle for each location \n",
    "ds = rm_annualcycle(ds, ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(forecast_time = 0).plot(col = 'realization', col_wrap = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(forecast_time = 0).mean('realization').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_valid = obs_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "\n",
    "obs_train = obs_2000_2019.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "obs_valid_ = rm_annualcycle(obs_valid, obs_train).sel(lead_time = lead)\n",
    "obs_valid_.isel(forecast_time = 0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conclusion\n",
    "The forecast fields and the observations deviate quite a bit, which limits the accuracy of the ANN probability predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove annual cycle from obs using train forecasts.\n",
    "#not the best thing to do..., forecasts are biased!\n",
    "annual_cycle = add_year_week_coords(fct_train)\n",
    "annual_cycle = annual_cycle.groupby('week').mean(['forecast_time','realization'])\n",
    "\n",
    "(obs_valid.isel(forecast_time = 0).sel(lead_time = lead) - annual_cycle.sel(week = 1, lead_time = lead)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verif_valid.sel(lead_time = lead).isel(forecast_time = 0).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute RPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes RPSS wrt climatology (1/3 for each category. So, negative RPSS are worse than climatology...\n",
    "\n",
    "def skill_by_year_single(prediction, terciled_obs):\n",
    "    \"\"\"version of skill_by_year adjusted to one var and one lead time and flexibel validation period\"\"\"\n",
    "    fct_p = prediction\n",
    "    obs_p = terciled_obs\n",
    "\n",
    "\n",
    "    # climatology\n",
    "    clim_p = xr.DataArray([1/3, 1/3, 1/3], dims='category', coords={'category':['below normal', 'near normal', 'above normal']}).to_dataset(name='tp')\n",
    "    clim_p['t2m'] = clim_p['tp']\n",
    "\n",
    "    clim_p = clim_p[v]\n",
    "\n",
    "    ## RPSS\n",
    "    # rps_ML\n",
    "    rps_ML = xs.rps(obs_p, fct_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    # rps_clim\n",
    "    rps_clim = xs.rps(obs_p, clim_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "\n",
    "    # rpss\n",
    "    rpss = 1 - (rps_ML / rps_clim)\n",
    "\n",
    "    # https://renkulab.io/gitlab/aaron.spring/s2s-ai-challenge-template/-/issues/7\n",
    "\n",
    "    # penalize\n",
    "    penalize = obs_p.where(fct_p!=1, other=-10).mean('category')\n",
    "    rpss = rpss.where(penalize!=0, other=-10)\n",
    "\n",
    "    # clip\n",
    "    rpss = rpss.clip(-10, 1)\n",
    "\n",
    "    # average over all forecasts\n",
    "    rpss_year = rpss.groupby('forecast_time.year').mean()\n",
    "\n",
    "    # weighted area mean\n",
    "    weights = np.cos(np.deg2rad(np.abs(rpss_year.latitude)))\n",
    "    # spatially weighted score averaged over lead_times and variables to one single value\n",
    "    scores = rpss_year.sel(latitude=slice(None, -60)).weighted(weights).mean('latitude').mean('longitude')\n",
    "    #scores = scores.to_array().mean(['lead_time', 'variable'])\n",
    "\n",
    "    return scores.to_dataframe('RPSS') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = xr_predicted_bins.reindex(latitude=xr_predicted_bins.latitude[::-1])\n",
    "#prediction.t2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_by_year_single(prediction.t2m, \n",
    "                      verif_valid.sel(lead_time = lead))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The RPSS of this approach is clearly higher than for the ensemble post-processing approach. Thus, the ANN predicting tercile probabilities outperforms the ensemble post-processing approach.\n",
    "However, this is probably because this approach predicts the smoothest fields and relaxes the most towards climatology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://phoenixnap.com/kb/linux-commands-check-memory-usage\n",
    "!free -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
