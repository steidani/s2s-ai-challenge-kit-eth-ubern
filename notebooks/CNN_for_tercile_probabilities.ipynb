{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML model to correct predictions of week 3-4 & 5-6\n",
    "\n",
    "This notebook create a Machine Learning `ML_model` to predict weeks 3-4 & 5-6 based on `S2S` weeks 3-4 & 5-6 forecasts and is compared to `CPC` observations for the [`s2s-ai-challenge`](https://s2s-ai-challenge.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: `ML-based mean bias reduction`\n",
    "\n",
    "- calculate the ML-based bias from 2000-2019 deterministic ensemble mean forecast\n",
    "- remove that the ML-based bias from 2020 forecast deterministic ensemble mean forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used\n",
    "\n",
    "type: renku datasets\n",
    "\n",
    "Training-input for Machine Learning model:\n",
    "- hindcasts of models:\n",
    "    - ECMWF: `ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr`\n",
    "\n",
    "Forecast-input for Machine Learning model:\n",
    "- real-time 2020 forecasts of models:\n",
    "    - ECMWF: `ecmwf_forecast-input_2020_biweekly_deterministic.zarr`\n",
    "\n",
    "Compare Machine Learning model forecast against against ground truth:\n",
    "- `CPC` observations:\n",
    "    - `hindcast-like-observations_biweekly_deterministic.zarr`\n",
    "    - `forecast-like-observations_2020_biweekly_deterministic.zarr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources used\n",
    "for training, details in reproducibility\n",
    "\n",
    "- platform: renku\n",
    "- memory: 8 GB\n",
    "- processors: 2 CPU\n",
    "- storage required: 10 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safeguards\n",
    "\n",
    "All points have to be [x] checked. If not, your submission is invalid.\n",
    "\n",
    "Changes to the code after submissions are not possible, as the `commit` before the `tag` will be reviewed.\n",
    "(Only in exceptions and if previous effort in reproducibility can be found, it may be allowed to improve readability and reproducibility after November 1st 2021.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safeguards to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting?wprov=sfti1) \n",
    "\n",
    "If the organizers suspect overfitting, your contribution can be disqualified.\n",
    "\n",
    "  - [x] We did not use 2020 observations in training (explicit overfitting and cheating)\n",
    "  - [x] We did not repeatedly verify my model on 2020 observations and incrementally improved my RPSS (implicit overfitting)\n",
    "  - [x] We provide RPSS scores for the training period with script `print_RPS_per_year`, see in section 6.3 `predict`.\n",
    "  - [x] We tried our best to prevent [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)?wprov=sfti1).\n",
    "  - [x] We honor the `train-validate-test` [split principle](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets). This means that the hindcast data is split into `train` and `validate`, whereas `test` is withheld.\n",
    "  - [x] We did not use `test` explicitly in training or implicitly in incrementally adjusting parameters.\n",
    "  - [x] We considered [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safeguards for Reproducibility\n",
    "Notebook/code must be independently reproducible from scratch by the organizers (after the competition), if not possible: no prize\n",
    "  - [x] All training data is publicly available (no pre-trained private neural networks, as they are not reproducible for us)\n",
    "  - [x] Code is well documented, readable and reproducible.\n",
    "  - [x] Code to reproduce training and predictions is preferred to run within a day on the described architecture. If the training takes longer than a day, please justify why this is needed. Please do not submit training piplelines, which take weeks to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos to improve template\n",
    "\n",
    "This is just a demo.\n",
    "\n",
    "- [ ] use multiple predictor variables and two predicted variables\n",
    "- [ ] for both `lead_time`s in one go\n",
    "- [ ] consider seasonality, for now all `forecast_time` months are mixed\n",
    "- [ ] make probabilistic predictions with `category` dim, for now works deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "xr.set_options(display_style='text')\n",
    "\n",
    "\n",
    "\n",
    "from dask.utils import format_bytes\n",
    "import xskillscore as xs\n",
    "\n",
    "%matplotlib inline \n",
    "#so that figures appear again\n",
    "\n",
    "\n",
    "#for prediction\n",
    "from scripts import make_probabilistic\n",
    "from scripts import add_valid_time_from_forecast_reference_time_and_lead_time\n",
    "from scripts import skill_by_year\n",
    "from scripts import add_year_week_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../template/data' #if you change this you also have to adjust the git lfs pull paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training data\n",
    "\n",
    "preprocessing of input data may be done in separate notebook/script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindcast\n",
    "\n",
    "get weekly initialized hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed as renku dataset\n",
    "!git lfs pull ../template/data/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = xr.open_zarr(f'{cache_path}/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "corresponding to hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed as renku dataset\n",
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019 = xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr', consolidated=True)#[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_terciled.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019_terciled = xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_terciled.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select region\n",
    "\n",
    "to make life easier for the beginning --> no periodic padding needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = slice(90,0)\n",
    "lon = slice(0,90) #negative,positive will not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = hind_2000_2019.sel(longitude = lon, latitude = lat)\n",
    "obs_2000_2019 = obs_2000_2019.sel(longitude = lon, latitude = lat)\n",
    "obs_2000_2019_terciled = obs_2000_2019_terciled.sel(longitude = lon, latitude = lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019.t2m.isel(lead_time = 0, forecast_time = 0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time is the forecast_time\n",
    "time_train_start,time_train_end='2000','2017' # train\n",
    "time_valid_start,time_valid_end='2018','2019' # valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weatherbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on [Weatherbench](https://github.com/pangeo-data/WeatherBench/blob/master/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once only and dont commit\n",
    "#!git clone https://github.com/pangeo-data/WeatherBench/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'WeatherBench')\n",
    "from WeatherBench.src.train_nn import PeriodicConv2D, create_predictions#DataGenerator, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v='t2m'\n",
    "bs=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://s2s-ai-challenge.github.io/\n",
    "\n",
    "We deal with two fundamentally different variables here: \n",
    "- Total precipitation is precipitation flux pr accumulated over lead_time until valid_time and therefore describes a point observation. \n",
    "- 2m temperature is averaged over lead_time(valid_time) and therefore describes an average observation. \n",
    "\n",
    "The submission file data model unifies both approaches and assigns 14 days for week 3-4 and 28 days for week 5-6 marking the first day of the biweekly aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 bi-weekly `lead_time`: week 3-4\n",
    "lead = hind_2000_2019.lead_time[0]\n",
    "#lead.values\n",
    "#lead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask: same missing values at all forecast_times\n",
    "mask = xr.where(obs_2000_2019.notnull(),1,np.nan).mean('forecast_time', skipna = False)\n",
    "#mask.isel(lead_time = 0).t2m.plot()\n",
    "\n",
    "#What if hind contains nan?--> precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "fct_train = hind_2000_2019.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "verif_train = obs_2000_2019_terciled.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "\n",
    "fct_train = fct_train.where(mask[v].notnull())\n",
    "verif_train = verif_train.where(mask[v].notnull())\n",
    "\n",
    "fct_train_mean = fct_train.mean('forecast_time')\n",
    "fct_train_std = fct_train.std('forecast_time')\n",
    "\n",
    "verif_train_mean = verif_train.mean('forecast_time')\n",
    "verif_train_std = verif_train.std('forecast_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "fct_valid = hind_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "verif_valid = obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "\n",
    "fct_valid = fct_valid.where(mask[v].notnull())\n",
    "verif_valid = verif_valid.where(mask[v].notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove annual cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_annualcycle(ds, ds_train):\n",
    "    #remove annual cycle for each location \n",
    "    \n",
    "    ds = add_year_week_coords(ds)\n",
    "    ds_train = add_year_week_coords(ds_train)\n",
    "    \n",
    "    if 'realization' in ds_train.coords:#always use train data to compute the annual cycle\n",
    "        ens_mean = ds_train.mean('realization')\n",
    "    else:\n",
    "        ens_mean = ds_train\n",
    "\n",
    "    ds_stand = ds - ens_mean.groupby('week').mean(['forecast_time'])\n",
    "\n",
    "    ds_stand = ds_stand.sel({'week' : ds.coords['week']})\n",
    "    ds_stand = ds_stand.drop(['week','year'])\n",
    "    ds_stand\n",
    "    return ds_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###remove annual cycle here\n",
    "fct_train = rm_annualcycle(fct_train, fct_train)\n",
    "fct_valid = rm_annualcycle(fct_valid, fct_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use ensemble mean as input\n",
    "fct_train = fct_train.mean('realization')#.isel(realization = 0)\n",
    "fct_valid = fct_valid.mean('realization')#.isel(realization = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct_train.sel(lead_time = lead).isel(forecast_time = 0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_train.sel(lead_time = lead).isel(forecast_time = 0).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create basis functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#lat lon grid of training input fields\n",
    "lats = fct_train.latitude\n",
    "lons = fct_train.longitude"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(lats))\n",
    "print(len(lons))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_bins = 3 #we have to predict probs for 3 bins\n",
    "r_basis = 14 #radius of support of basis functions\n",
    "dist_basis = r_basis/2 #distance between centers of basis functions\n",
    "\n",
    "#number of basis functions\n",
    "n_basis = int(np.ceil((lats[0] - lats[-1])/dist_basis + 1)*np.ceil((lons[-1] - lons[0])/dist_basis + 1))\n",
    "\n",
    "#grid coords\n",
    "lon_np = lons\n",
    "lat_np = lats\n",
    "length_lon = len(lon_np)\n",
    "length_lat = len(lat_np)\n",
    "\n",
    "lon_np = np.outer(lon_np, np.ones(length_lat)).reshape(int(length_lon * length_lat))\n",
    "lat_np = np.outer(lat_np, np.ones(length_lon)).reshape(int(length_lon * length_lat))\n",
    "lon_np.shape\n",
    "\n",
    "#number of grid points\n",
    "n_xy = int(length_lon*length_lat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#centers of basis functions\n",
    "lon_ctr = np.arange(lons[0],lons[-1] + dist_basis,dist_basis)\n",
    "length_lon_ctr = len(lon_ctr) #number of center points in lon direction\n",
    "\n",
    "lat_ctr = np.arange(lats[0],lats[-1] - dist_basis,- dist_basis)\n",
    "length_lat_ctr = len(lat_ctr) #number of center points in lat direction\n",
    "\n",
    "lon_ctr = np.outer(lon_ctr, np.ones(length_lat_ctr)).reshape(int(n_basis))\n",
    "lat_ctr = np.outer(np.ones(length_lon_ctr), lat_ctr).reshape(int(n_basis))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#compute distances between fct grid and basis function centers\n",
    "dst_lon = np.abs(np.subtract.outer(lon_np,lon_ctr).reshape(len(lons),len(lats),n_basis))#10,14\n",
    "dst_lon = np.swapaxes(dst_lon, 0, 1)\n",
    "dst_lat = np.abs(np.subtract.outer(lat_np,lat_ctr).reshape(len(lats),len(lons),n_basis))#'14,10'\n",
    "\n",
    "dst = np.sqrt(dst_lon**2+dst_lat**2)\n",
    "dst = np.swapaxes(dst, 0, 1).reshape(n_xy,n_basis)\n",
    "\n",
    "dst.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#define basis functions\n",
    "\n",
    "basis = np.where(dst>r_basis,0.,(1.-(dst/r_basis)**3)**3)#main step, zero outside, \n",
    "basis = basis/np.sum(basis,axis=1)[:,None]#normalization at each grid point\n",
    "nbs = basis.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basis(inp_field, r_basis):\n",
    "    \"\"\"returns a set of basis functions for the input field, adapted from Scheuerer et al. 2020.\n",
    "\n",
    "    PARAMETERS:\n",
    "    inp_field : (xarray DataArray) basis functions for these lat lon coords will be created\n",
    "    r_basis : (int) radius of support of basis functions, \n",
    "                    the distance between centers of basis functions is half this radius,\n",
    "                    should be choosen depending on input field size.\n",
    "    \n",
    "    RETURNS:\n",
    "    basis : \n",
    "    lats : lats of input field\n",
    "    lons : lons of input field\n",
    "    n_xy : number of grid points in input field\n",
    "    n_basis : number of basis functions\n",
    "    \"\"\"  \n",
    "    \n",
    "    #r_basis = 14 #radius of support of basis functions\n",
    "    dist_basis = r_basis/2 #distance between centers of basis functions\n",
    "\n",
    "    lats = inp_field.latitude\n",
    "    lons = inp_field.longitude\n",
    "    \n",
    "    #number of basis functions\n",
    "    n_basis = int(np.ceil((lats[0] - lats[-1])/dist_basis + 1)*np.ceil((lons[-1] - lons[0])/dist_basis + 1))\n",
    "\n",
    "    #grid coords\n",
    "    lon_np = lons\n",
    "    lat_np = lats\n",
    "    \n",
    "    length_lon = len(lon_np)\n",
    "    length_lat = len(lat_np)\n",
    "    \n",
    "    lon_np = np.outer(lon_np, np.ones(length_lat)).reshape(int(length_lon * length_lat))\n",
    "    lat_np = np.outer(lat_np, np.ones(length_lon)).reshape(int(length_lon * length_lat))\n",
    "\n",
    "    #number of grid points\n",
    "    n_xy = int(length_lon*length_lat)\n",
    "\n",
    "    #centers of basis functions\n",
    "    lon_ctr = np.arange(lons[0],lons[-1] + dist_basis,dist_basis)\n",
    "    length_lon_ctr = len(lon_ctr) #number of center points in lon direction\n",
    "\n",
    "    lat_ctr = np.arange(lats[0],lats[-1] - dist_basis,- dist_basis)\n",
    "    length_lat_ctr = len(lat_ctr) #number of center points in lat direction\n",
    "\n",
    "    lon_ctr = np.outer(lon_ctr, np.ones(length_lat_ctr)).reshape(int(n_basis))\n",
    "    lat_ctr = np.outer(np.ones(length_lon_ctr), lat_ctr).reshape(int(n_basis))\n",
    "\n",
    "    #compute distances between fct grid and basis function centers\n",
    "    dst_lon = np.abs(np.subtract.outer(lon_np,lon_ctr).reshape(len(lons),len(lats),n_basis))#10,14\n",
    "    dst_lon = np.swapaxes(dst_lon, 0, 1)\n",
    "    dst_lat = np.abs(np.subtract.outer(lat_np,lat_ctr).reshape(len(lats),len(lons),n_basis))#'14,10'\n",
    "\n",
    "    dst = np.sqrt(dst_lon**2+dst_lat**2)\n",
    "    dst = np.swapaxes(dst, 0, 1).reshape(n_xy,n_basis)\n",
    "\n",
    "    #define basis functions\n",
    "    basis = np.where(dst>r_basis,0.,(1.-(dst/r_basis)**3)**3)#main step, zero outside, \n",
    "    basis = basis/np.sum(basis,axis=1)[:,None]#normalization at each grid point\n",
    "    nbs = basis.shape[1]\n",
    "    \n",
    "    return basis, lats, lons, n_xy, n_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis, lats, lons, n_xy, n_basis = get_basis(fct_train, 14) #30\n",
    "##the smaller you choose the radius of the basis functions, the more memory needs to be allocated (a lot more!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Reshape, Dot, Add, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameters for CNN\n",
    "\n",
    "n_bins = 3 #we have to predict probs for 3 bins\n",
    "dropout_rate = 0.4\n",
    "hidden_nodes = [10] #they tried different architectures\n",
    "clim_probs = np.log(1/3)*np.ones((n_xy,n_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN: slightly adapted from Scheuerer et al 2020.\n",
    "\n",
    "inp_imgs = Input(shape=(len(lats),len(lons),1,)) #fcts\n",
    "inp_basis = Input(shape=(n_xy,n_basis)) #basis\n",
    "inp_cl = Input(shape=(n_xy,n_bins,)) #climatology\n",
    "\n",
    "c = Conv2D(4, (3,3), activation='elu')(inp_imgs)\n",
    "c = MaxPooling2D((2,2))(c)\n",
    "c = Conv2D(8, (3,3), activation='elu')(c)\n",
    "c = MaxPooling2D((2,2))(c)\n",
    "x = Flatten()(c)\n",
    "for h in hidden_nodes: \n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(h, activation='elu')(x)\n",
    "x = Dense(n_bins*n_basis, activation='elu')(x)\n",
    "x = Reshape((n_bins,n_basis))(x)\n",
    "z = Dot(axes=2)([inp_basis, x])     # Tensor product with basis functions\n",
    "z = Add()([z, inp_cl])              # Add (log) probability anomalies to log climatological probabilities \n",
    "out = Activation('softmax')(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Model(inputs=[inp_imgs, inp_basis, inp_cl], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=keras.optimizers.Adam(1e-4))#'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow import set_random_seed\n",
    "from tensorflow import random\n",
    "random.set_seed(1)#this seems not to have any effect so far..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit([fct_train.sel(lead_time = lead).transpose('forecast_time',...).fillna(0.).values,  \n",
    "         np.repeat(basis[np.newaxis,:,:],len(fct_train.forecast_time),axis=0),\n",
    "         np.repeat(clim_probs[np.newaxis,:,:],len(fct_train.forecast_time),axis=0)],\n",
    "        verif_train.sel(lead_time = lead).stack(Z = ['latitude','longitude']).transpose('forecast_time','Z',...).fillna(0.).values, \n",
    "        epochs=5, shuffle = True,\n",
    "        validation_data =([fct_valid.transpose('forecast_time',...).sel(lead_time = lead).values,\n",
    "                          np.repeat(basis[np.newaxis,:,:],len(fct_valid.forecast_time),axis=0),\n",
    "                          np.repeat(clim_probs[np.newaxis,:,:],len(fct_valid.forecast_time),axis=0)],\n",
    "                         verif_valid.sel(lead_time = lead).stack(Z = ['latitude','longitude']).transpose('forecast_time','Z',...).values)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cnn.evaluate([fct_valid.transpose('forecast_time',...).sel(lead_time = lead).fillna(0.).values,\n",
    "                        np.repeat(basis[np.newaxis,:,:],len(fct_valid.forecast_time),axis=0),\n",
    "                       np.repeat(clim_probs[np.newaxis,:,:],len(fct_valid.forecast_time),axis=0)],\n",
    "                       verif_valid.sel(lead_time = lead).stack(Z = ['latitude','longitude']).transpose('forecast_time','Z',...).fillna(0.).values\n",
    "                       )\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some ideas for improvements:\n",
    "#circular input\n",
    "#https://www.tu-chemnitz.de/etit/proaut/publications/schubert19_IV.pdf\n",
    "#https://www.tu-chemnitz.de/etit/proaut/en/research/ccnn.html\n",
    "#add indices\n",
    "#https://stackoverflow.com/questions/47818968/adding-an-additional-value-to-a-convolutional-neural-network-input\n",
    "#https://datascience.stackexchange.com/questions/68450/how-can-you-include-information-not-present-in-an-image-for-neural-networks\n",
    "#https://www.nature.com/articles/s41598-019-42294-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define create_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_predictions(model, dg, lead):\n",
    "    \"\"\"Create non-iterative predictions\n",
    "    returns: prediction in the shape of the input arguments to DataGenerator classe\n",
    "    \"\"\"\n",
    "    import tensorflow as tf \n",
    "    \n",
    "    preds = model.predict(dg).squeeze()\n",
    "    \n",
    "    preds = Reshape((len(lons),len(lats),3))(preds)\n",
    "    preds = tf.transpose(preds, [0,3,1,2])\n",
    "    \n",
    "    #transform to original shape of input\n",
    "    #if dg.verif_dataset:\n",
    "    #    da = xr.DataArray(\n",
    "    #                preds,\n",
    "    #                dims=['forecast_time', 'latitude', 'longitude','variable'],\n",
    "    #                coords={'forecast_time': fct_valid.forecast_time, 'latitude': fct_train.latitude,\n",
    "    #                        'longitude': fct_train.longitude},\n",
    "    #            ).to_dataset() # doesnt work yet\n",
    "    #else:\n",
    "    da = xr.DataArray(\n",
    "                preds,\n",
    "                dims=['forecast_time', 'category','longitude', 'latitude'],\n",
    "                coords={'forecast_time': fct_valid.forecast_time, 'category' : verif_train.category, 'latitude': fct_train.latitude,\n",
    "                        'longitude': fct_train.longitude}\n",
    "            )\n",
    "    da = da.transpose('forecast_time','category','latitude',...)\n",
    "    da = da.assign_coords(lead_time=lead)\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_valid_time_single(forecast, init_dim='forecast_time'):\n",
    "    \"\"\"Creates valid_time(forecast_time, lead_time) for a single lead time and variable\n",
    "    \n",
    "    lead_time: pd.Timedelta\n",
    "    forecast_time: datetime\n",
    "    \"\"\"\n",
    "    times = xr.DataArray(\n",
    "                forecast[init_dim] + lead,\n",
    "                dims=init_dim,\n",
    "                coords={init_dim: forecast[init_dim]},\n",
    "            )\n",
    "            \n",
    "    forecast = forecast.assign_coords(valid_time=times)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_prediction(cnn, dg, lead):\n",
    "    \"\"\"prediction for one var and one lead-time\n",
    "    \n",
    "    args:\n",
    "    time: time slice\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    preds_test = _create_predictions(cnn, dg, lead)\n",
    "    \n",
    "    # add valid_time coord\n",
    "    ###preds_test = add_valid_time_from_forecast_reference_time_and_lead_time(preds_test)# only works for complete output\n",
    "    preds_test = add_valid_time_single(preds_test)\n",
    "    #preds_test = preds_test.to_dataset(name=v)\n",
    "                                    \n",
    "    return preds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction from CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_single = single_prediction(cnn, \n",
    "                                 [fct_valid.sel(lead_time = lead).transpose('forecast_time',...).fillna(0.).values,  \n",
    "                                   np.repeat(basis[np.newaxis,:,:],len(fct_valid.forecast_time),axis=0),\n",
    "                                 np.repeat(clim_probs[np.newaxis,:,:],len(fct_valid.forecast_time),axis=0)],\n",
    "                                  lead)\n",
    "        #verif_train.sel(lead_time = lead).transpose('forecast_time','longitude'...).fillna(0.).values,                         \n",
    "                                  #fct_valid.sel(lead_time = lead).fillna(0.).values,np.repeat(basis[np.newaxis,:,:],len(fct_valid.forecast_time),axis=0)], \n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_masked = preds_single.where(mask[v].sel(lead_time = lead).notnull())\n",
    "preds_masked.isel(forecast_time = 0).plot(col = 'category', vmin = 0, vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only show probability of most likeliest tercile for each grid cell\n",
    "#preds_masked_one_hot = xr.where(preds_masked == preds_masked.max('category'),preds_masked,0)\n",
    "#preds_masked_one_hot.where(mask[v].sel(lead_time = lead).notnull()).isel(forecast_time = 0).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_valid.sel(lead_time = lead).isel(forecast_time = 0).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_valid.sel(lead_time = lead).isel(forecast_time = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tercile probs of the raw ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "#tercile_edges : used in create_predictions --> make_probabilistic\n",
    "#mask: used in make_probabilistic, but make_probabilistic would also work without mask\n",
    "\n",
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc\n",
    "tercile_file = f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc'\n",
    "tercile_edges = xr.open_dataset(tercile_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = hind_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "test_raw = make_probabilistic(test, tercile_edges)\n",
    "test_raw.isel(lead_time = 0).isel(forecast_time = 0)['t2m'].where(mask[v].sel(lead_time = lead).notnull()).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check: probs of terciles add up to one\n",
    "#preds_single.isel(forecast_time = 0).sum('category').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute RPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes RPSS wrt climatology (1/3 for each category. So, negative RPSS are worse than climatology...\n",
    "\n",
    "def skill_by_year_single(prediction, terciled_obs):\n",
    "    \"\"\"version of skill_by_year adjusted to one var and one lead time and flexibel validation period\"\"\"\n",
    "    fct_p = prediction\n",
    "    obs_p = terciled_obs\n",
    "\n",
    "\n",
    "    # climatology\n",
    "    clim_p = xr.DataArray([1/3, 1/3, 1/3], dims='category', coords={'category':['below normal', 'near normal', 'above normal']}).to_dataset(name='tp')\n",
    "    clim_p['t2m'] = clim_p['tp']\n",
    "\n",
    "    clim_p = clim_p[v]\n",
    "\n",
    "    ## RPSS\n",
    "    # rps_ML\n",
    "    rps_ML = xs.rps(obs_p, fct_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    # rps_clim\n",
    "    rps_clim = xs.rps(obs_p, clim_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "\n",
    "    # rpss\n",
    "    rpss = 1 - (rps_ML / rps_clim)\n",
    "\n",
    "    # https://renkulab.io/gitlab/aaron.spring/s2s-ai-challenge-template/-/issues/7\n",
    "\n",
    "    # penalize\n",
    "    penalize = obs_p.where(fct_p!=1, other=-10).mean('category')\n",
    "    rpss = rpss.where(penalize!=0, other=-10)\n",
    "\n",
    "    # clip\n",
    "    rpss = rpss.clip(-10, 1)\n",
    "\n",
    "    # average over all forecasts\n",
    "    rpss_year = rpss.groupby('forecast_time.year').mean()\n",
    "\n",
    "    # weighted area mean\n",
    "    weights = np.cos(np.deg2rad(np.abs(rpss_year.latitude)))\n",
    "    # spatially weighted score averaged over lead_times and variables to one single value\n",
    "    scores = rpss_year.sel(latitude=slice(None, -60)).weighted(weights).mean('latitude').mean('longitude')\n",
    "    #scores = scores.to_array().mean(['lead_time', 'variable'])\n",
    "\n",
    "    return scores.to_dataframe('RPSS') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].sel(lead_time = lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_by_year_single(preds_single, \n",
    "                     obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].sel(lead_time = lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPSS in the order of -0.015 (for r_basis = 14)\n",
    "# if you choose a larger radius for the basis functions, we might come closer to climatology and hence achieve a better RPSS.\n",
    "# fluctuations in predictions are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_by_year_single(test_raw[v].sel(lead_time = lead),\n",
    "                     obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].sel(lead_time = lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPSS in the order of -0.635"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The RPSS of this CNN approach is higher than for all ANNs. \n",
    "This is probably because this approach predicts the smoothest fields and relaxes the most towards climatology.\n",
    "Actually, the CNN fields are less smooth than the fields of ANN terciled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://phoenixnap.com/kb/linux-commands-check-memory-usage\n",
    "!free -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
