{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create biweekly renku datasets from `climetlab-s2s-ai-challenge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapted from renku_datasets_biweekly\n",
    "\n",
    "* downloads climetlab data\n",
    "* computes weekly and biweekly averages and saves it to zarr according to challenge naming conventions. File names are appended by the variable name to avoid overwriting existing files.\n",
    "\n",
    "For me: run this in s2s-ai-new3-copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information:\n",
    "\n",
    "https://confluence.ecmwf.int/display/S2S/Parameters\n",
    "\n",
    "https://github.com/ecmwf-lab/climetlab-s2s-ai-challenge\n",
    "\n",
    "download settings: \n",
    "\n",
    "https://climetlab.readthedocs.io/en/latest/guide/settings.html\n",
    "\n",
    "in particular you can change timeout time when downloading from an url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "import pandas as pd\n",
    "\n",
    "import climetlab_s2s_ai_challenge\n",
    "import climetlab as cml\n",
    "print(f'Climetlab version : {cml.__version__}')\n",
    "print(f'Climetlab-s2s-ai-challenge plugin version : {climetlab_s2s_ai_challenge.__version__}')\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "xr.set_options(display_style='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching path for climetlab\n",
    "cache_path_cml = \"/work/S2S_AI/cache4\"#'../../Data/s2s_ai/cache'## set your own path\n",
    "cml.settings.set(\"cache-directory\", cache_path_cml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"../data\"#'../../Data/s2s_ai'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and cache\n",
    "\n",
    "Download all files for the observations, forecast and hindcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut\n",
    "#from scripts import download\n",
    "#download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hindcast and forecast `input`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adapt below to your variable (adapt both, cache_path_cml and varlist_forecast)\n",
    "#### first check the size of the data here: https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/training-input/0.3.0/netcdf/index.html\n",
    "small data sets use 40GB of cache, larger up to 300GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist_forecast = ['sst']#z500 #['tp','t2m'] # can add more\n",
    "\n",
    "# caching path for climetlab\n",
    "# set your own path: I created a new folder for each variable\n",
    "cache_path_cml = \"/work/S2S_AI/cache4/sst\"#'../../Data/s2s_ai/cache/sm20' \n",
    "cml.settings.set(\"cache-directory\", cache_path_cml)\n",
    "\n",
    "center_list = ['ecmwf'] # 'ncep', 'eccc'\n",
    "\n",
    "#forecast_dataset_labels = ['training-input','test-input'] # ML community\n",
    "# equiv to\n",
    "forecast_dataset_labels = ['hindcast-input','forecast-input'] # NWP community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes ~ 10-30 min to download for one model one variable depending on number of model realizations\n",
    "# and download settings https://climetlab.readthedocs.io/en/latest/guide/settings.html \n",
    "for center in center_list:\n",
    "    for ds in forecast_dataset_labels:\n",
    "        print(ds)\n",
    "        cml.load_dataset(f\"s2s-ai-challenge-{ds}\", origin=center, parameter=varlist_forecast, format='netcdf').to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## observations `output-reference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist_obs = ['tp']#['tp', 't2m']#tcw\n",
    "\n",
    "# starting dates forecast_time in 2020\n",
    "dates = xr.cftime_range(start='20200102',freq='7D', periods=53).strftime('%Y%m%d').to_list()\n",
    "\n",
    "obs_dataset_labels = ['training-output-reference','test-output-reference'] # ML community\n",
    "# equiv to\n",
    "obs_dataset_labels = ['hindcast-like-observations','forecast-like-observations'] # NWP community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### we are mostly interested in downloading more variables for the input...\n",
    "#%%time\n",
    "# takes 10min to download\n",
    "#for ds in obs_dataset_labels:\n",
    "#    print(ds)\n",
    "#    # only netcdf, no format choice\n",
    "#    cml.load_dataset(f\"s2s-ai-challenge-{ds}\", date=dates, parameter=varlist_obs).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download obs_time for to create output-reference/observations for other models than ecmwf and eccc,\n",
    "# i.e. ncep or any S2S or Sub model\n",
    "#obs_time = cml.load_dataset(f\"s2s-ai-challenge-observations\", parameter=['t2m', 'pr']).to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create bi-weekly aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import aggregate_biweekly, ensure_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(center_list)\n",
    "print(obs_dataset_labels)\n",
    "print(varlist_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biweekly\n",
    "\n",
    "change \"dsl in ...\"-line to aggregate obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, center in enumerate(center_list):  # forecast centers (could also take models)\n",
    "    for dsl in forecast_dataset_labels:  # climetlab dataset labels #obs_dataset_labels:# +  change this line depending on whether you want to download obs or fct\n",
    "        #read in data to ds\n",
    "        for p, parameter in enumerate(varlist_forecast):  # variables\n",
    "            if c != 0 and 'observation' in dsl:  # only do once for observations \n",
    "                continue\n",
    "            print(f\"datasetlabel: {dsl}, center: {center}, parameter: {parameter}\")\n",
    "            \n",
    "            #forecasts/hindcasts\n",
    "            if 'input' in dsl:\n",
    "                ds = cml.load_dataset(f\"s2s-ai-challenge-{dsl}\", origin=center, parameter=parameter, format='netcdf').to_xarray()\n",
    "            #obs\n",
    "            elif 'observation' in dsl: # obs only netcdf, no choice\n",
    "                if parameter not in ['t2m', 'tp']:\n",
    "                    continue\n",
    "                ds = cml.load_dataset(f\"s2s-ai-challenge-{dsl}\", parameter=parameter, date=dates).to_xarray()\n",
    "\n",
    "            if p == 0: #first variable\n",
    "                ds_biweekly = ds.map(aggregate_biweekly)\n",
    "            else:\n",
    "                #here you do the biweekly aggregation #add more vars\n",
    "                ds_biweekly[parameter] = ds.map(aggregate_biweekly)[parameter]#map: apply function to each variable in dataset\n",
    "\n",
    "            ds_biweekly = ds_biweekly.map(ensure_attributes, biweekly=True)\n",
    "            \n",
    "            ds_biweekly = ds_biweekly.sortby('forecast_time')\n",
    "    #ds is now ready\n",
    "        if 'test' in dsl:\n",
    "            ds_biweekly = ds_biweekly.chunk('auto')\n",
    "        else:\n",
    "            ds_biweekly = ds_biweekly.chunk({'forecast_time':'auto','lead_time':-1,'longitude':-1,'latitude':-1})\n",
    "\n",
    "        if 'hindcast' in dsl:\n",
    "            time = f'{int(ds_biweekly.forecast_time.dt.year.min())}-{int(ds_biweekly.forecast_time.dt.year.max())}'\n",
    "            if 'input' in dsl:\n",
    "                name = f'{center}_{dsl}'\n",
    "            elif 'observations':\n",
    "                name = dsl\n",
    "\n",
    "        elif 'forecast' in dsl:\n",
    "            time = '2020'\n",
    "            if 'input' in dsl:\n",
    "                name = f'{center}_{dsl}'\n",
    "            elif 'observations':\n",
    "                name = dsl\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        # pattern: {model_if_not_observations}{observations/forecast/hindcast}_{time}_biweekly_deterministic.zarr\n",
    "        params = '_'.join(varlist_forecast)\n",
    "        zp = f'{cache_path}/{name}_{time}_biweekly_deterministic_{params}.zarr'\n",
    "        ds_biweekly.attrs.update({'postprocessed_by':'https://renkulab.io/gitlab/aaron.spring/s2s-ai-challenge-template/-/blob/master/notebooks/renku_datasets_biweekly.ipynb'})\n",
    "        print(f'save to: {zp}')\n",
    "        ds_biweekly.astype('float32').to_zarr(zp, consolidated=True, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create weekly aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import add_valid_time_from_forecast_reference_time_and_lead_time\n",
    "def aggregate_weekly(da):\n",
    "    \"\"\"\n",
    "    Aggregate initialized S2S forecasts weekly for xr.DataArrays.\n",
    "    Use ds.map(aggregate_weekly) for xr.Datasets.\n",
    "    \n",
    "    This function does not return values for week 1, \n",
    "    since lead_time = 0 is not available for certain variables (e.g. t2m) or is zero (e.g. tp).\n",
    "    \n",
    "    Applies to the ECMWF S2S data model: https://confluence.ecmwf.int/display/S2S/Parameters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    da : xarray dataarray with time coordinate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    da_weekly : xarray dataarray containing weekly aggregated quantities\n",
    "    \"\"\"\n",
    "    #da = da.assign_coords({'forecast_time': da.time[0].values})\n",
    "    #da = da.expand_dims('forecast_time')\n",
    "    #da = da.assign_coords({'lead_time': da.time - da.time[0]})\n",
    "    #da = da.swap_dims({'time': 'lead_time'})\n",
    "\n",
    "    #da = da.assign_coords({'forecast_time': da.time[0].values})\n",
    "    #da = da.assign_coords({'lead_time': da.time - da.time[0]})\n",
    "    #da = da.swap_dims({'time': 'lead_time'})\n",
    "    #da = da.assign_coords({'forecast_time': da.time[0]})\n",
    "    #da = da.assign_coords({'lead_time': da.time - da.time[0]})\n",
    "    \n",
    "    # weekly averaging\n",
    "    #w1 = [pd.Timedelta(f'{i} d') for i in range(0,7)]\n",
    "    #w1 = xr.DataArray(w1,dims='lead_time', coords={'lead_time':w1})\n",
    "    \n",
    "    w2 = [pd.Timedelta(f'{i} d') for i in range(7,14)]\n",
    "    w2 = xr.DataArray(w2,dims='lead_time', coords={'lead_time':w2})\n",
    "    \n",
    "    w3 = [pd.Timedelta(f'{i} d') for i in range(14,21)]\n",
    "    w3 = xr.DataArray(w3,dims='lead_time', coords={'lead_time':w3})\n",
    "    \n",
    "    w4 = [pd.Timedelta(f'{i} d') for i in range(21,28)]\n",
    "    w4 = xr.DataArray(w4,dims='lead_time', coords={'lead_time':w4})\n",
    "    \n",
    "    w4 = [pd.Timedelta(f'{i} d') for i in range(21,28)]\n",
    "    w4 = xr.DataArray(w4,dims='lead_time', coords={'lead_time':w4})\n",
    "    \n",
    "    w5 = [pd.Timedelta(f'{i} d') for i in range(28,35)]\n",
    "    w5 = xr.DataArray(w5,dims='lead_time', coords={'lead_time':w5})\n",
    "    \n",
    "    w6 = [pd.Timedelta(f'{i} d') for i in range(35,42)]\n",
    "    w6 = xr.DataArray(w6,dims='lead_time', coords={'lead_time':w6})\n",
    "    \n",
    "    \n",
    "    weekly_lead = [pd.Timedelta(f\"{i} d\") for i in [7, 14, 21, 28, 35,]] # take first day of weekly average as new coordinate\n",
    "\n",
    "    v = da.name\n",
    "    if v in ['tp', 'ttr']:#climetlab_s2s_ai_challenge.CF_CELL_METHODS[v] == 'sum': # weekly difference for sum variables: tp and ttr\n",
    "        #d1 = da.sel(lead_time=pd.Timedelta(\"7 d\")) - da.sel(lead_time=pd.Timedelta(\"0 d\"))\n",
    "        d2 = da.sel(lead_time=pd.Timedelta(\"14 d\")) - da.sel(lead_time=pd.Timedelta(\"7 d\"))\n",
    "        d3 = da.sel(lead_time=pd.Timedelta(\"21 d\")) - da.sel(lead_time=pd.Timedelta(\"14 d\"))\n",
    "        d4 = da.sel(lead_time=pd.Timedelta(\"28 d\")) - da.sel(lead_time=pd.Timedelta(\"21 d\"))\n",
    "        d5 = da.sel(lead_time=pd.Timedelta(\"35 d\")) - da.sel(lead_time=pd.Timedelta(\"28 d\"))\n",
    "        d6 = da.sel(lead_time=pd.Timedelta(\"42 d\")) - da.sel(lead_time=pd.Timedelta(\"35 d\"))\n",
    "        \n",
    "        #d34 = da.sel(lead_time=pd.Timedelta(\"28 d\")) - da.sel(lead_time=pd.Timedelta(\"14 d\")) # tp from day 14 to day 27\n",
    "        #d56 = da.sel(lead_time=pd.Timedelta(\"42 d\")) - da.sel(lead_time=pd.Timedelta(\"28 d\")) # tp from day 28 to day 42\n",
    "        \n",
    "        #da_weekly = xr.concat([d1,d2,d3,d4,d5,d6],'lead_time').assign_coords(lead_time=biweekly_lead)\n",
    "        \n",
    "    else: # t2m, see climetlab_s2s_ai_challenge.CF_CELL_METHODS # biweekly: mean [day 14, day 27]\n",
    "        #d1 = da.sel(lead_time=w1).mean('lead_time')\n",
    "        d2 = da.sel(lead_time=w2).mean('lead_time')\n",
    "        d3 = da.sel(lead_time=w3).mean('lead_time')\n",
    "        d4 = da.sel(lead_time=w4).mean('lead_time')\n",
    "        d5 = da.sel(lead_time=w5).mean('lead_time')\n",
    "        d6 = da.sel(lead_time=w6).mean('lead_time')\n",
    "        \n",
    "        #d34 = da.sel(lead_time=w34).mean('lead_time')\n",
    "        #d56 = da.sel(lead_time=w56).mean('lead_time')\n",
    "        \n",
    "    da_weekly = xr.concat([d2,d3,d4,d5,d6],'lead_time').assign_coords(lead_time=weekly_lead)\n",
    "    \n",
    "    da_weekly = add_valid_time_from_forecast_reference_time_and_lead_time(da_weekly)\n",
    "    da_weekly['lead_time'].attrs = {'long_name':'forecast_period', 'description': 'Forecast period is the time interval between the forecast reference time and the validity time.',\n",
    "                         'aggregate': 'The pd.Timedelta corresponds to the first day of a weekly aggregate.',\n",
    "                         'week34_t2m': 'mean[day 14, 27]',\n",
    "                         'week56_t2m': 'mean[day 28, 41]',\n",
    "                         'week34_tp': 'day 28 minus day 14',\n",
    "                         'week56_tp': 'day 42 minus day 28'}\n",
    "    \n",
    "    return da_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute and save weekly aggregates\n",
    "#cache data for all parameters must be available.\n",
    "#all variables are loaded again if you do this for several variables at once\n",
    "\n",
    "for c, center in enumerate(center_list):  # forecast centers (could also take models)\n",
    "    for dsl in forecast_dataset_labels:  # climetlab dataset labels #obs_dataset_labels:# +  change this line depending on whether you want to download obs or fct\n",
    "        #read in data to ds\n",
    "        for p, parameter in enumerate(varlist_forecast):  # variables\n",
    "            if c != 0 and 'observation' in dsl:  # only do once for observations \n",
    "                continue\n",
    "            print(f\"datasetlabel: {dsl}, center: {center}, parameter: {parameter}\")\n",
    "            \n",
    "            #forecasts/hindcasts\n",
    "            if 'input' in dsl:\n",
    "                ds = cml.load_dataset(f\"s2s-ai-challenge-{dsl}\", origin=center, parameter=parameter, format='netcdf').to_xarray()\n",
    "            #obs\n",
    "            elif 'observation' in dsl: # obs only netcdf, no choice\n",
    "                if parameter not in ['t2m', 'tp']:\n",
    "                    continue\n",
    "                ds = cml.load_dataset(f\"s2s-ai-challenge-{dsl}\", parameter=parameter, date=dates).to_xarray()\n",
    "\n",
    "            if p == 0: #first variable\n",
    "                ds_weekly = ds.map(aggregate_weekly)\n",
    "            else:\n",
    "                #here you do the biweekly aggregation #add more vars\n",
    "                ds_weekly[parameter] = ds.map(aggregate_weekly)[parameter]#map: apply function to each variable in dataset\n",
    "\n",
    "            ds_weekly = ds_weekly.map(ensure_attributes, biweekly=False)\n",
    "            \n",
    "            ds_weekly = ds_weekly.sortby('forecast_time')\n",
    "    #ds is now ready\n",
    "        if 'test' in dsl:\n",
    "            ds_weekly = ds_weekly.chunk('auto')\n",
    "        else:\n",
    "            ds_weekly = ds_weekly.chunk({'forecast_time':'auto','lead_time':-1,'longitude':-1,'latitude':-1})\n",
    "\n",
    "        if 'hindcast' in dsl:\n",
    "            time = f'{int(ds_weekly.forecast_time.dt.year.min())}-{int(ds_weekly.forecast_time.dt.year.max())}'\n",
    "            if 'input' in dsl:\n",
    "                name = f'{center}_{dsl}'\n",
    "            elif 'observations':\n",
    "                name = dsl\n",
    "\n",
    "        elif 'forecast' in dsl:\n",
    "            time = '2020'\n",
    "            if 'input' in dsl:\n",
    "                name = f'{center}_{dsl}'\n",
    "            elif 'observations':\n",
    "                name = dsl\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        # pattern: {model_if_not_observations}{observations/forecast/hindcast}_{time}_biweekly_deterministic.zarr\n",
    "        params = '_'.join(varlist_forecast)\n",
    "        zp = f'{cache_path}/{name}_{time}_weekly_deterministic_{params}.zarr'\n",
    "        ds_weekly.attrs.update({'postprocessed_by':'https://renkulab.io/gitlab/aaron.spring/s2s-ai-challenge-template/-/blob/master/notebooks/renku_datasets_biweekly.ipynb'})\n",
    "        print(f'save to: {zp}')\n",
    "        ds_weekly.astype('float32').to_zarr(zp, consolidated=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_w_hindcast = ds_hindcast.map(aggregate_weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try out cml.load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = ['t2m']#varlist_forecast\n",
    "\n",
    "#this part is quick once the data has been downloaded to the cache\n",
    "#dsl: forecast_dataset_labels = ['hindcast-input','forecast-input']\n",
    "ds_hindcast = cml.load_dataset(f\"s2s-ai-challenge-{'hindcast-input'}\", origin='ecmwf', parameter=parameter, format='netcdf').to_xarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hindcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tp\n",
    "ds_hindcast_tp = cml.load_dataset(f\"s2s-ai-challenge-{'hindcast-input'}\", origin='ecmwf', parameter='tp', format='netcdf').to_xarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hindcast_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hindcast.isel(forecast_time = 0).mean(('realization','latitude','longitude')).t2m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validate weekly aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = xr.open_zarr(f'{cache_path}/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019_weekly = xr.open_zarr(f'{cache_path}/ecmwf_hindcast-input_2000-2019_weekly_deterministictp_t2m.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019_weekly.isel(lead_time = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tp 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biweekly_from_weekly_tp_34 = hind_2000_2019_weekly.tp.isel(lead_time = 1) +hind_2000_2019_weekly.tp.isel(lead_time = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biweekly_from_weekly_tp_34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hind_2000_2019.isel(lead_time = 0).tp - biweekly_from_weekly_tp_34).sum(('realization', 'forecast_time')).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tp 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biweekly_from_weekly_tp_56 = hind_2000_2019_weekly.tp.isel(lead_time = 3) +hind_2000_2019_weekly.tp.isel(lead_time = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hind_2000_2019.isel(lead_time = 1).tp - biweekly_from_weekly_tp_56).sum(('realization', 'forecast_time')).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t2m 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biweekly_from_weekly_t2m_34 = (hind_2000_2019_weekly.t2m.isel(lead_time = 1) + hind_2000_2019_weekly.t2m.isel(lead_time = 2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hind_2000_2019.isel(lead_time = 0).t2m - biweekly_from_weekly_t2m_34).sum(('realization', 'forecast_time')).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biweekly_from_weekly_t2m_34.mean(('realization', 'forecast_time')).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t2m 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biweekly_from_weekly_t2m_56 = (hind_2000_2019_weekly.t2m.isel(lead_time = 3) +hind_2000_2019_weekly.t2m.isel(lead_time = 4))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hind_2000_2019.isel(lead_time = 1).t2m - biweekly_from_weekly_t2m_56).sum(('realization', 'forecast_time')).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
