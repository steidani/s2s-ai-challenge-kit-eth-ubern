{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5093045d",
   "metadata": {},
   "source": [
    "# Train ML model to correct predictions of week 3-4 & 5-6\n",
    "\n",
    "This notebook create a Machine Learning `ML_model` to predict weeks 3-4 & 5-6 based on `S2S` weeks 3-4 & 5-6 forecasts and is compared to `CPC` observations for the [`s2s-ai-challenge`](https://s2s-ai-challenge.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac0b37",
   "metadata": {},
   "source": [
    "# Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542581ad",
   "metadata": {},
   "source": [
    "## Method: `ML-based mean bias reduction`\n",
    "\n",
    "- calculate the ML-based bias from 2000-2019 deterministic ensemble mean forecast\n",
    "- remove that the ML-based bias from 2020 forecast deterministic ensemble mean forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ffd09",
   "metadata": {},
   "source": [
    "## Data used\n",
    "\n",
    "type: renku datasets\n",
    "\n",
    "Training-input for Machine Learning model:\n",
    "- hindcasts of models:\n",
    "    - ECMWF: `ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr`\n",
    "\n",
    "Forecast-input for Machine Learning model:\n",
    "- real-time 2020 forecasts of models:\n",
    "    - ECMWF: `ecmwf_forecast-input_2020_biweekly_deterministic.zarr`\n",
    "\n",
    "Compare Machine Learning model forecast against against ground truth:\n",
    "- `CPC` observations:\n",
    "    - `hindcast-like-observations_biweekly_deterministic.zarr`\n",
    "    - `forecast-like-observations_2020_biweekly_deterministic.zarr`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782379f7",
   "metadata": {},
   "source": [
    "## Resources used\n",
    "for training, details in reproducibility\n",
    "\n",
    "- platform: renku\n",
    "- memory: 8 GB\n",
    "- processors: 2 CPU\n",
    "- storage required: 10 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9408ff",
   "metadata": {},
   "source": [
    "## Safeguards\n",
    "\n",
    "All points have to be [x] checked. If not, your submission is invalid.\n",
    "\n",
    "Changes to the code after submissions are not possible, as the `commit` before the `tag` will be reviewed.\n",
    "(Only in exceptions and if previous effort in reproducibility can be found, it may be allowed to improve readability and reproducibility after November 1st 2021.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a64dd",
   "metadata": {},
   "source": [
    "### Safeguards to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting?wprov=sfti1) \n",
    "\n",
    "If the organizers suspect overfitting, your contribution can be disqualified.\n",
    "\n",
    "  - [x] We did not use 2020 observations in training (explicit overfitting and cheating)\n",
    "  - [x] We did not repeatedly verify my model on 2020 observations and incrementally improved my RPSS (implicit overfitting)\n",
    "  - [x] We provide RPSS scores for the training period with script `print_RPS_per_year`, see in section 6.3 `predict`.\n",
    "  - [x] We tried our best to prevent [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)?wprov=sfti1).\n",
    "  - [x] We honor the `train-validate-test` [split principle](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets). This means that the hindcast data is split into `train` and `validate`, whereas `test` is withheld.\n",
    "  - [x] We did not use `test` explicitly in training or implicitly in incrementally adjusting parameters.\n",
    "  - [x] We considered [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4898693",
   "metadata": {},
   "source": [
    "### Safeguards for Reproducibility\n",
    "Notebook/code must be independently reproducible from scratch by the organizers (after the competition), if not possible: no prize\n",
    "  - [x] All training data is publicly available (no pre-trained private neural networks, as they are not reproducible for us)\n",
    "  - [x] Code is well documented, readable and reproducible.\n",
    "  - [x] Code to reproduce training and predictions is preferred to run within a day on the described architecture. If the training takes longer than a day, please justify why this is needed. Please do not submit training piplelines, which take weeks to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7193c",
   "metadata": {},
   "source": [
    "# Todos to improve template\n",
    "\n",
    "This is just a demo.\n",
    "\n",
    "- [ ] use multiple predictor variables and two predicted variables\n",
    "- [ ] for both `lead_time`s in one go\n",
    "- [ ] consider seasonality, for now all `forecast_time` months are mixed\n",
    "- [ ] make probabilistic predictions with `category` dim, for now works deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98aedad",
   "metadata": {},
   "source": [
    "# Description of this notebook\n",
    "\n",
    "* ANN that takes ensemble mean as input and returns a post-processed version of the input field (also a field, no terciles)\n",
    "* ANN output is used by make_probabilistic to create tercile probabilities. In particular, each ensemble member is individually fed to the ANN, resulting in a post-processed ensemble. The post-processed ensemble is used to compute the tercile probabilities using make_probabilistic.\n",
    "* additional features (week of year, lat/lon): no clear improvement in loss\n",
    "* separate function for removing the annual cycle\n",
    "* investigate annual cycle that is removed in the pre-processing\n",
    "* standardization is done after preprocessing (always using train data)\n",
    "* investigates what the model did learn: Model adjusts the fct to the obs (likely accounting for different representations of orography), though most of this adjustment is already done by the pre-/post-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53583f4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xarray as xr\n",
    "xr.set_options(display_style='text')\n",
    "\n",
    "\n",
    "\n",
    "from dask.utils import format_bytes\n",
    "import xskillscore as xs\n",
    "\n",
    "%matplotlib inline \n",
    "#for figures\n",
    "\n",
    "#for prediction\n",
    "from scripts import make_probabilistic\n",
    "from scripts import add_valid_time_from_forecast_reference_time_and_lead_time\n",
    "from scripts import skill_by_year\n",
    "from scripts import add_year_week_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a99b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../template/data' #if you change this you also have to adjust the git lfs pull paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7074fd5e",
   "metadata": {},
   "source": [
    "# Get training data\n",
    "\n",
    "preprocessing of input data may be done in separate notebook/script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157a919",
   "metadata": {},
   "source": [
    "## Hindcast\n",
    "\n",
    "get weekly initialized hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c307f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed as renku dataset\n",
    "!git lfs pull ../template/data/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c58f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = xr.open_zarr(f'{cache_path}/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad14387",
   "metadata": {},
   "source": [
    "## Observations\n",
    "corresponding to hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f72fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed as renku dataset\n",
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019 = xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr', consolidated=True)#[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c5d87",
   "metadata": {},
   "source": [
    "terciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_terciled.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019_terciled = xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_terciled.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc17bf",
   "metadata": {},
   "source": [
    "### Select region\n",
    "\n",
    "to make life easier for the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = slice(90,0)\n",
    "lon = slice(0,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = hind_2000_2019.sel(longitude = lon, latitude = lat)\n",
    "obs_2000_2019 = obs_2000_2019.sel(longitude = lon, latitude = lat)\n",
    "obs_2000_2019_terciled = obs_2000_2019_terciled.sel(longitude = lon, latitude = lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e274ea4",
   "metadata": {},
   "source": [
    "2020 data contains the same amount of nan as 2018 and 2019 data.\n",
    "the gridcells with partly missing values in obs are harder to predict, therefore taking the validation to be 2018, 2019 will lead to lower validation loss but high training loss. --> Make sure to have the same nans in all years. Differences in loss also due to different standardization (remove annual cycle with only two years, will make the problem much easier, than if annual cycle based on 18 years is removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c8066f",
   "metadata": {},
   "source": [
    "## Train Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time is the forecast_time\n",
    "time_train_start,time_train_end='2000','2017' # train#2017\n",
    "time_valid_start,time_valid_end='2018','2019' # valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ae271",
   "metadata": {},
   "source": [
    "## Weatherbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1760089",
   "metadata": {},
   "source": [
    "based on [Weatherbench](https://github.com/pangeo-data/WeatherBench/blob/master/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once only and dont commit\n",
    "#!git clone https://github.com/pangeo-data/WeatherBench/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.insert(1, 'WeatherBench')\n",
    "#from WeatherBench.src.train_nn import PeriodicConv2D, create_predictions#DataGenerator, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe12692",
   "metadata": {},
   "source": [
    "### define some vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b890e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v='t2m'\n",
    "bs=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e004aed",
   "metadata": {},
   "source": [
    "https://s2s-ai-challenge.github.io/\n",
    "\n",
    "We deal with two fundamentally different variables here: \n",
    "- Total precipitation is precipitation flux pr accumulated over lead_time until valid_time and therefore describes a point observation. \n",
    "- 2m temperature is averaged over lead_time(valid_time) and therefore describes an average observation. \n",
    "\n",
    "The submission file data model unifies both approaches and assigns 14 days for week 3-4 and 28 days for week 5-6 marking the first day of the biweekly aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 bi-weekly `lead_time`: week 3-4\n",
    "lead = hind_2000_2019.lead_time[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec70d2",
   "metadata": {},
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fdd0c0",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe5a05",
   "metadata": {},
   "source": [
    "define mask to have the same missing values at all forecast_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = xr.where(obs_2000_2019.notnull(),1,np.nan).mean('forecast_time', skipna = False)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.t2m.sel(lead_time = lead).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "fct_valid = hind_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].mean('realization')\n",
    "verif_valid = obs_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "\n",
    "fct_valid = fct_valid.where(mask[v].notnull())\n",
    "verif_valid = verif_valid.where(mask[v].notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train:\n",
    "#uses only ensemble mean so far\n",
    "fct_train = hind_2000_2019.sel(forecast_time=slice(time_train_start,time_train_end))[v].mean('realization')\n",
    "verif_train = obs_2000_2019.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "\n",
    "fct_train = fct_train.where(mask[v].notnull())\n",
    "verif_train = verif_train.where(mask[v].notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#orange: number of missing values without masking, blue: with masking\n",
    "fct_nans = xr.where(np.isnan(fct_train), 1, 0)\n",
    "fct_nans.sum(('lead_time', 'latitude', 'longitude')).plot()\n",
    "obs_nans = xr.where(np.isnan(obs_2000_2019), 1, 0)\n",
    "obs_nans.t2m.sum(('lead_time', 'latitude', 'longitude')).plot()\n",
    "#mask does what it should"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df45ea",
   "metadata": {},
   "source": [
    "### Annual cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254dd261",
   "metadata": {},
   "source": [
    "#### obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting takes some time\n",
    "ds = verif_train.sel(lead_time = lead)\n",
    "ds_train = verif_train.sel(lead_time = lead)\n",
    "\n",
    "ds = add_year_week_coords(ds)\n",
    "ds_train = add_year_week_coords(ds_train)\n",
    "\n",
    "if 'realization' in ds.coords:\n",
    "    ens_mean = ds_train.mean('realization')\n",
    "else:\n",
    "    ens_mean = ds_train\n",
    "\n",
    "annual_cycle = ens_mean.groupby('week').mean(['forecast_time'])\n",
    "annual_cycle = annual_cycle.stack(z = ('latitude','longitude'))\n",
    "\n",
    "#reset_inex to avoit error message\n",
    "annual_cycle = annual_cycle.reset_index(\"z\")\n",
    "#https://github.com/pydata/xarray/pull/3938\n",
    "annual_cycle.plot(hue='z', x=\"week\", add_legend = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4fd02",
   "metadata": {},
   "source": [
    "#### ensemble forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d638592",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = fct_train.sel(lead_time = lead)\n",
    "ds_train = fct_train.sel(lead_time = lead)\n",
    "\n",
    "ds = add_year_week_coords(ds)\n",
    "ds_train = add_year_week_coords(ds_train)\n",
    "\n",
    "if 'realization' in ds.coords:\n",
    "    ens_mean = ds_train.mean('realization')\n",
    "else:\n",
    "    ens_mean = ds_train\n",
    "\n",
    "annual_cycle = ens_mean.groupby('week').mean(['forecast_time'])\n",
    "stacked = annual_cycle.stack(z = ('latitude','longitude'))\n",
    "\n",
    "stacked = stacked.reset_index(\"z\")\n",
    "stacked.plot(hue='z', x=\"week\", add_legend = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with removed annual cycle: example year 2000\n",
    "\n",
    "ds_stand = (ds - annual_cycle)\n",
    "ds_stand = ds_stand.sel({'week' : ds.coords['week']})\n",
    "ds_stand = ds_stand.drop(['week','year','valid_time'])\n",
    "\n",
    "ds_stand = ds_stand.sel(forecast_time = '2000')\n",
    "\n",
    "ds_stand = ds_stand.stack(z = ('latitude','longitude'))\n",
    "ds_stand = ds_stand.reset_index(\"z\")\n",
    "ds_stand.plot(hue='z', x=\"forecast_time\", add_legend = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3c1be",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_annualcycle(ds, ds_train):\n",
    "    #remove annual cycle for each location \n",
    "    \n",
    "    ds = add_year_week_coords(ds)\n",
    "    ds_train = add_year_week_coords(ds_train)\n",
    "    \n",
    "    if 'realization' in ds_train.coords:\n",
    "        ens_mean = ds_train.mean('realization')\n",
    "    else:\n",
    "        ens_mean = ds_train\n",
    "\n",
    "    ds_stand = ds - ens_mean.groupby('week').mean(['forecast_time'])#always use train to remove annual cycle\n",
    "\n",
    "    ds_stand = ds_stand.sel({'week' : ds.coords['week']})\n",
    "    ds_stand_ = ds_stand.drop(['week','year'])\n",
    " \n",
    "    return ds_stand_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce203a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_preprocess(ds, ds_train, v,lead):\n",
    "    ds = ds.sel(lead_time = lead)\n",
    "    ds_train = ds_train.sel(lead_time = lead)\n",
    " \n",
    "    ds = rm_annualcycle(ds, ds_train)\n",
    "        \n",
    "    #provide time feature\n",
    "    week = add_year_week_coords(ds)\n",
    "    week_ = np.cos(2*np.pi/53*(week.week +53/2))\n",
    "    week_ = week_.drop(['week','year'])\n",
    "    week_ = week_.expand_dims({'longitude': ds.longitude, 'latitude': ds.latitude})\n",
    "    \n",
    "    var = ds.drop(['week','year']).to_dataset(name = '{}'.format(v))\n",
    "    week_ = week_.to_dataset(name = 'week')\n",
    "    combined = xr.combine_by_coords([var, week_])\n",
    "\n",
    "    df = combined.to_dataframe()\n",
    "    df = df.drop(['lead_time','valid_time'], axis =1).reset_index()\n",
    "    df = df.dropna(axis = 0)\n",
    "    \n",
    "    df_ref = df\n",
    "    \n",
    "    df = df.drop(['forecast_time'], axis = 1)\n",
    "    return df, df_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9562cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_preprocess_label(ds,ds_train,v,lead):\n",
    "    ds = ds.sel(lead_time = lead)\n",
    "    ds_train = ds_train.sel(lead_time = lead)\n",
    "    \n",
    "    ds = rm_annualcycle(ds, ds_train)\n",
    "    \n",
    "    df = ds.to_dataframe()\n",
    "    df = df.drop(['lead_time','valid_time'], axis =1).reset_index()\n",
    "    \n",
    "    df = df.dropna(axis = 0)\n",
    "    \n",
    "    df=df.drop(['forecast_time','latitude','longitude'], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataframes\n",
    "df_verif_train = ann_preprocess_label(verif_train, verif_train, v, lead)\n",
    "df_fct_train, df_fct_train_ref = ann_preprocess(fct_train, fct_train, v, lead)\n",
    "df_verif_valid = ann_preprocess_label(verif_valid, verif_train, v, lead)\n",
    "df_fct_valid, df_fct_valid_ref = ann_preprocess(fct_valid, fct_train, v, lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af06d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_verif_train.mean(axis = 0))\n",
    "print(df_verif_train.std(axis = 0))\n",
    "print(df_fct_train.mean(axis = 0))\n",
    "print(df_fct_train.std(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d35587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_verif_valid.mean(axis = 0))\n",
    "print(df_verif_valid.std(axis = 0))\n",
    "print(df_fct_valid.mean(axis = 0))\n",
    "print(df_fct_valid.std(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize input and output\n",
    "\n",
    "mean_verif_train = df_verif_train.mean(axis = 0)\n",
    "std_verif_train = df_verif_train.std(axis = 0)\n",
    "mean_fct_train = df_fct_train.mean(axis = 0)\n",
    "std_fct_train = df_fct_train.std(axis = 0)\n",
    "\n",
    "#validation set using train mean and std\n",
    "df_verif_valid = (df_verif_valid - mean_verif_train)/std_verif_train\n",
    "df_fct_valid   = (df_fct_valid - mean_fct_train)/std_fct_train\n",
    "\n",
    "df_verif_train = (df_verif_train - mean_verif_train)/std_verif_train\n",
    "df_fct_train   = (df_fct_train - mean_fct_train)/std_fct_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d25852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verif_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fct_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e352a1",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cabef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "\n",
    "ann = keras.models.Sequential([\n",
    "    Dense(100, input_shape=(4,), activation='relu'),\n",
    "    #Dropout(0.2),\n",
    "    Dense(100,  activation='relu'),\n",
    "    #Activation('softmax')\n",
    "    Dense(1,activation='linear'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(loss='mse', optimizer=keras.optimizers.Adam(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037cf1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.fit(df_fct_train, df_verif_train, batch_size = 1024, epochs=5, validation_data=(df_fct_valid, df_verif_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5660d5e",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_output(output, df_ref, ds_input, verif_train, v, mean_verif_train, std_verif_train):\n",
    "    \n",
    "    #add columns\n",
    "    output['latitude'] = df_ref.latitude.values\n",
    "    output['longitude'] = df_ref.longitude.values\n",
    "    output['forecast_time'] = df_ref.forecast_time.values\n",
    "    \n",
    "    output[v] = output[v]*std_verif_train[v]  + mean_verif_train[v]#undo standardization \n",
    "    ##shold be done using train\n",
    "    \n",
    "    #create MultiIndex\n",
    "    output = output.pivot_table(values = v, index = ['latitude','longitude','forecast_time'])\n",
    "    \n",
    "    #convert to dataset\n",
    "    xr_output = xr.Dataset.from_dataframe(output)\n",
    "    \n",
    "    #retain the complete coords    \n",
    "    temp = ds_input.sel(lead_time = lead).drop(['valid_time','lead_time'])\n",
    "    temp = temp.to_dataset(name = 'zeros')\n",
    "    merged = xr.merge([xr_output, temp])\n",
    "    merged = merged.drop('zeros')\n",
    "\n",
    "    #add annual cycle\n",
    "    annual_cycle = add_year_week_coords(verif_train)\n",
    "    if 'realization' in verif_train.coords:\n",
    "        annual_cycle = annual_cycle.groupby('week').mean(['forecast_time']).mean('realization')  \n",
    "    else:\n",
    "        annual_cycle = annual_cycle.groupby('week').mean(['forecast_time'])\n",
    "\n",
    "    pred = add_year_week_coords(merged)\n",
    "    pred = pred + annual_cycle.sel(lead_time = lead)\n",
    "    pred = pred.sel({'week' : merged.coords['week']})\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_member(data, fct_train, v, lead, df_verif_train, verif_train, ann, \n",
    "              mean_verif_train, std_verif_train, mean_fct_train,std_fct_train):\n",
    "\n",
    "    #preprocess i.e convert to pandas\n",
    "    df_real, df_ref = ann_preprocess(data,fct_train, v, lead)\n",
    "\n",
    "    #standardize using train data\n",
    "    df_real  = (df_real - mean_fct_train)/std_fct_train\n",
    "    \n",
    "    #predict plus add column headers\n",
    "    pred = pd.DataFrame(ann.predict(df_real), columns = df_verif_train.columns)\n",
    "    #convert back to xarray and add annual cycle\n",
    "    pred__ = postprocess_output(pred, df_ref, data, verif_train, v, mean_verif_train, std_verif_train)\n",
    "    \n",
    "    return pred__\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load validation forecasts (as ensemble and not ensemble mean, model is still trained on ens mean)\n",
    "test = hind_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15aca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for all ensemble members\n",
    "preds = []\n",
    "for i in test.realization.values:\n",
    "    pred = ml_member(test.sel(realization = i).drop('realization'), \n",
    "                     fct_train, v, lead, df_verif_train, verif_train, ann,\n",
    "                     mean_verif_train, std_verif_train, mean_fct_train,std_fct_train)\n",
    "    pred = pred.assign_coords(realization = i)\n",
    "    preds.append(pred)\n",
    "    \n",
    "preds = xr.concat(preds, dim = 'realization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b3fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0d2ef",
   "metadata": {},
   "source": [
    "### Make probability forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc\n",
    "tercile_edges = xr.open_dataset(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aced59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tercile_edges = tercile_edges.sel(longitude = lon, latitude = lat)\n",
    "tercile_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##double lead st make_prob works\n",
    "#make_prob also for second lead time, since two dim lead-time vector is wanted by make probabilistic\n",
    "preds__ = preds.reset_coords('lead_time').drop('lead_time')\n",
    "preds_1 = preds__.assign_coords(lead_time = test.lead_time[0])\n",
    "preds_2 = preds__.assign_coords(lead_time = test.lead_time[1])\n",
    "    \n",
    "preds_new = xr.concat([preds_1, preds_2], dim = 'lead_time')\n",
    "preds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6030985",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_preds = make_probabilistic(preds_new.isel(forecast_time = 0).t2m, tercile_edges)\n",
    "obs_preds.t2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_preds.isel(lead_time = 0)['t2m'].where(mask[v].sel(lead_time = lead).notnull()).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019_terciled.sel(forecast_time = '2018-01-02', lead_time = lead).t2m.plot(col = 'category', figsize=(10, 6),cbar_kwargs={'orientation': 'horizontal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019_terciled.sel(forecast_time = '2018-01-02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346885ac",
   "metadata": {},
   "source": [
    "### prob forecast of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = make_probabilistic(test, tercile_edges)\n",
    "test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw.isel(lead_time = 0).isel(forecast_time = 0)['t2m'].where(mask[v].sel(lead_time = lead).notnull()).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1dde5",
   "metadata": {},
   "source": [
    "### prob forecast of test passed through pipeline without ml model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "###shows how to undo all the transformations\n",
    "### i.e. returns the input\n",
    "\n",
    "def ml_member_without_ml(data, fct_train, v, lead, df_verif_train, verif_train, ann, mean_verif_train, std_verif_train):\n",
    "\n",
    "    #preprocess i.e convert to pandas\n",
    "    df_real, df_ref = ann_preprocess(data,fct_train, v, lead)#remove ann cycle fct #orig pipeline\n",
    "    \n",
    "    #standardize using train fct data\n",
    "    df_real  = (df_real - mean_fct_train)/std_fct_train #orig pipeline\n",
    "    \n",
    "    #predict plus add column headers\n",
    "    #pred = df_real #pred = pd.DataFrame(ann.predict(df_real), columns = df_verif_train.columns)\n",
    "    \n",
    "    #pred = pred*std_fct_train + mean_fct_train #undo forecast standardization\n",
    "    \n",
    "    #pred = (pred - mean_verif_train)/std_verif_train # do obs stand\n",
    "    \n",
    "    pred = df_real\n",
    "    #convert back to xarray and add annual cycle\n",
    "    pred_ = postprocess_output(pred, df_ref, data, verif_train, v, mean_verif_train, std_verif_train)# orig pipeline\n",
    "    #undo obs stand\n",
    "    #add ann obs\n",
    "    \n",
    "    #pred_ = rm_annualcycle(pred_, verif_train.sel(lead_time = lead))\n",
    "\n",
    "\n",
    "    #add annual cycle\n",
    "    #annual_cycle = add_year_week_coords(fct_train)\n",
    "    #if 'realization' in verif_train.coords:\n",
    "    #    annual_cycle = annual_cycle.groupby('week').mean(['forecast_time']).mean('realization')  \n",
    "    #else:\n",
    "    #    annual_cycle = annual_cycle.groupby('week').mean(['forecast_time'])\n",
    "\n",
    "    #pred__ = add_year_week_coords(pred_)\n",
    "    #pred__ = pred__ + annual_cycle.sel(lead_time = lead)\n",
    "    #pred__ = pred__.sel({'week' : pred_.coords['week']})\n",
    "    \n",
    "    \n",
    "     #   pred = add_year_week_coords(merged)\n",
    "    #pred = pred + annual_cycle.sel(lead_time = lead)\n",
    "    #pred = pred.sel({'week' : merged.coords['week']})\n",
    "    \n",
    "    pred__ = pred_\n",
    "    return pred__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_raw = []\n",
    "for i in test.realization.values:\n",
    "    pred_raw = ml_member_without_ml(test.sel(realization = i).drop('realization'), \n",
    "                           fct_train, v, lead, df_verif_train, verif_train, ann, mean_verif_train, std_verif_train)\n",
    "    pred_raw = pred_raw.assign_coords(realization = i)\n",
    "    preds_raw.append(pred_raw)\n",
    "    \n",
    "preds_raw = xr.concat(preds_raw, dim = 'realization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b6dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###this is the transformation that the ANN has to learn at least\n",
    "\n",
    "(preds_raw.isel(forecast_time = 0).t2m - test.where(mask[v].notnull()).isel(forecast_time = 0).sel(lead_time = lead)).plot(col = 'realization', col_wrap = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254167ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fct_time = slice(0 +53,52 + 53,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw fct input minus obs\n",
    "\n",
    "(test.isel(forecast_time = fct_time).sel(lead_time = lead).mean('realization') - verif_valid.where(mask[v].notnull()).isel(forecast_time = fct_time).sel(lead_time = lead)).plot(col = 'forecast_time', col_wrap = 5, cmap='RdBu_r', vmin=-15, vmax=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be931d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##the ml model does indeed remove local forecast biases, although it is not able to capture all biases\n",
    "#biases caused by topography are mostly removed by the pre- and post-processing (see below), ml model contributes not much \n",
    "\n",
    "#ml transformed input minus obs\n",
    "(preds.isel(forecast_time = fct_time).t2m.mean('realization') - verif_valid.where(mask[v].notnull()).isel(forecast_time = fct_time).sel(lead_time = lead)).plot(col = 'forecast_time', col_wrap = 5,cmap='RdBu_r', vmin=-15, vmax=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ae8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformed input minus obs\n",
    "(preds_raw.isel(forecast_time = fct_time).t2m.mean('realization') - verif_valid.where(mask[v].notnull()).isel(forecast_time = fct_time).sel(lead_time = lead)).plot(col = 'forecast_time', col_wrap = 5,cmap='RdBu_r', vmin=-15, vmax=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71cb3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_1_raw = preds_raw.assign_coords(lead_time = test.lead_time[0])\n",
    "preds_2_raw = preds_raw.assign_coords(lead_time = test.lead_time[1])\n",
    "    \n",
    "preds_new_raw = xr.concat([preds_1_raw, preds_2_raw], dim = 'lead_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_preds_raw = make_probabilistic(preds_new_raw.isel(forecast_time = 0).t2m, tercile_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_preds_raw.isel(lead_time = 0)['t2m'].where(mask[v].sel(lead_time = lead).notnull()).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de22356",
   "metadata": {},
   "source": [
    "## Compute RPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21199594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skill_by_year_single(prediction, terciled_obs):\n",
    "    \"\"\"version of skill_by_year adjusted to one var and one lead time and flexibel validation period\"\"\"\n",
    "    fct_p = prediction\n",
    "    obs_p = terciled_obs\n",
    "\n",
    "\n",
    "    # climatology\n",
    "    clim_p = xr.DataArray([1/3, 1/3, 1/3], dims='category', coords={'category':['below normal', 'near normal', 'above normal']}).to_dataset(name='tp')\n",
    "    clim_p['t2m'] = clim_p['tp']\n",
    "\n",
    "    clim_p = clim_p[v]\n",
    "\n",
    "    ## RPSS\n",
    "    # rps_ML\n",
    "    rps_ML = xs.rps(obs_p, fct_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    # rps_clim\n",
    "    rps_clim = xs.rps(obs_p, clim_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "\n",
    "    # rpss\n",
    "    rpss = 1 - (rps_ML / rps_clim)\n",
    "\n",
    "    # https://renkulab.io/gitlab/aaron.spring/s2s-ai-challenge-template/-/issues/7\n",
    "\n",
    "    # penalize\n",
    "    penalize = obs_p.where(fct_p!=1, other=-10).mean('category')\n",
    "    rpss = rpss.where(penalize!=0, other=-10)\n",
    "\n",
    "    # clip\n",
    "    rpss = rpss.clip(-10, 1)\n",
    "\n",
    "    # average over all forecasts\n",
    "    rpss_year = rpss.groupby('forecast_time.year').mean()\n",
    "\n",
    "    # weighted area mean\n",
    "    weights = np.cos(np.deg2rad(np.abs(rpss_year.latitude)))\n",
    "    # spatially weighted score averaged over lead_times and variables to one single value\n",
    "    scores = rpss_year.sel(latitude=slice(None, -60)).weighted(weights).mean('latitude').mean('longitude')\n",
    "    #scores = scores.to_array().mean(['lead_time', 'variable'])\n",
    "\n",
    "    return scores.to_dataframe('RPSS') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ce38e",
   "metadata": {},
   "source": [
    "### RPSS of ML post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_by_year_single(obs_preds[v].sel(lead_time = lead), \n",
    "                     obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].sel(lead_time = lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffba00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPSS in the order of -0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497278bb",
   "metadata": {},
   "source": [
    "### RPSS of pre/post-processing without ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_by_year_single(prob_preds_raw.sel(lead_time = lead).t2m, \n",
    "                     obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].sel(lead_time = lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPSS in the order of -0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f75f5",
   "metadata": {},
   "source": [
    "### RPSS of raw ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_by_year_single(test_raw.sel(lead_time = lead).t2m, \n",
    "                     obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].sel(lead_time = lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPSS in the order of -0.635"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b89299",
   "metadata": {},
   "source": [
    "#### Post-processed ensemble achieved higher RPSS, but is still below baseline.\n",
    "The post-processing using the ml model was most successful, but also the simple pre-/post-processing (removing local annual cycle and feature standardization) did improve the RPSS compared to the raw ensemble.\n",
    "Here, only the climatology (1/3, 1/3, 1/3) is used as a baseline. The RPSS is always negative which means that all approaches are worse than climatology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3b28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
