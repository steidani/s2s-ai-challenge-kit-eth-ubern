{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML model to correct predictions of week 3-4 & 5-6\n",
    "\n",
    "This notebook create a Machine Learning `ML_model` to predict weeks 3-4 & 5-6 based on `S2S` weeks 3-4 & 5-6 forecasts and is compared to `CPC` observations for the [`s2s-ai-challenge`](https://s2s-ai-challenge.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: `ML-based mean bias reduction`\n",
    "\n",
    "- calculate the ML-based bias from 2000-2019 deterministic ensemble mean forecast\n",
    "- remove that the ML-based bias from 2020 forecast deterministic ensemble mean forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used\n",
    "\n",
    "type: renku datasets\n",
    "\n",
    "Training-input for Machine Learning model:\n",
    "- hindcasts of models:\n",
    "    - ECMWF: `ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr`\n",
    "\n",
    "Forecast-input for Machine Learning model:\n",
    "- real-time 2020 forecasts of models:\n",
    "    - ECMWF: `ecmwf_forecast-input_2020_biweekly_deterministic.zarr`\n",
    "\n",
    "Compare Machine Learning model forecast against against ground truth:\n",
    "- `CPC` observations:\n",
    "    - `hindcast-like-observations_biweekly_deterministic.zarr`\n",
    "    - `forecast-like-observations_2020_biweekly_deterministic.zarr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources used\n",
    "for training, details in reproducibility\n",
    "\n",
    "- platform: renku\n",
    "- memory: 8 GB\n",
    "- processors: 2 CPU\n",
    "- storage required: 10 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safeguards\n",
    "\n",
    "All points have to be [x] checked. If not, your submission is invalid.\n",
    "\n",
    "Changes to the code after submissions are not possible, as the `commit` before the `tag` will be reviewed.\n",
    "(Only in exceptions and if previous effort in reproducibility can be found, it may be allowed to improve readability and reproducibility after November 1st 2021.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safeguards to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting?wprov=sfti1) \n",
    "\n",
    "If the organizers suspect overfitting, your contribution can be disqualified.\n",
    "\n",
    "  - [x] We did not use 2020 observations in training (explicit overfitting and cheating)\n",
    "  - [x] We did not repeatedly verify my model on 2020 observations and incrementally improved my RPSS (implicit overfitting)\n",
    "  - [x] We provide RPSS scores for the training period with script `print_RPS_per_year`, see in section 6.3 `predict`.\n",
    "  - [x] We tried our best to prevent [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)?wprov=sfti1).\n",
    "  - [x] We honor the `train-validate-test` [split principle](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets). This means that the hindcast data is split into `train` and `validate`, whereas `test` is withheld.\n",
    "  - [x] We did not use `test` explicitly in training or implicitly in incrementally adjusting parameters.\n",
    "  - [x] We considered [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safeguards for Reproducibility\n",
    "Notebook/code must be independently reproducible from scratch by the organizers (after the competition), if not possible: no prize\n",
    "  - [x] All training data is publicly available (no pre-trained private neural networks, as they are not reproducible for us)\n",
    "  - [x] Code is well documented, readable and reproducible.\n",
    "  - [x] Code to reproduce training and predictions is preferred to run within a day on the described architecture. If the training takes longer than a day, please justify why this is needed. Please do not submit training piplelines, which take weeks to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos to improve template\n",
    "\n",
    "This is just a demo.\n",
    "\n",
    "- [ ] use multiple predictor variables and two predicted variables\n",
    "- [ ] for both `lead_time`s in one go\n",
    "- [ ] consider seasonality, for now all `forecast_time` months are mixed\n",
    "- [ ] make probabilistic predictions with `category` dim, for now works deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "xr.set_options(display_style='text')\n",
    "\n",
    "\n",
    "\n",
    "from dask.utils import format_bytes\n",
    "import xskillscore as xs\n",
    "\n",
    "%matplotlib inline \n",
    "#for figures\n",
    "\n",
    "\n",
    "#for prediction\n",
    "from scripts import make_probabilistic\n",
    "from scripts import add_valid_time_from_forecast_reference_time_and_lead_time\n",
    "from scripts import skill_by_year\n",
    "from scripts import add_year_week_coords\n",
    "\n",
    "\n",
    "from helper_ml_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training data\n",
    "\n",
    "preprocessing of input data may be done in separate notebook/script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'local_n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindcast\n",
    "\n",
    "get weekly initialized hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = load_data(data = 'hind_2000-2019', aggregation = 'biweekly', path = path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = hind_2000_2019.isel(depth_below_and_layer = 0 ).reset_coords('depth_below_and_layer', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "corresponding to hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019 = load_data(data = 'obs_2000-2019', aggregation = 'biweekly', path = path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs_2000_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019_terciled = load_data(data = 'obs_terciled_2000-2019', aggregation = 'biweekly', path = path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs_2000_2019_terciled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select region\n",
    "\n",
    "* use the whole globe as input since global teleconnections are a major source of predictability, at least over Europe.\n",
    "* create predictions for a much smaller domain, since otherwise the basis functions (and the associated multiplication) uses too much memory.\n",
    "\n",
    "to make life easier for the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lat = slice(90,-90)\n",
    "input_lon = slice(0, 360)\n",
    "\n",
    "output_lat = slice(90,0)\n",
    "output_lon = slice(0,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_2000_2019 = hind_2000_2019.sel(longitude = input_lon, latitude = input_lat)\n",
    "obs_2000_2019 = obs_2000_2019.sel(longitude = output_lon, latitude = output_lat)\n",
    "obs_2000_2019_terciled = obs_2000_2019_terciled.sel(longitude = output_lon, latitude = output_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019.t2m.isel(lead_time = 0, forecast_time = 0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time is the forecast_time\n",
    "time_train_start,time_train_end='2000','2017' # train\n",
    "time_valid_start,time_valid_end='2018','2019' # valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weatherbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on [Weatherbench](https://github.com/pangeo-data/WeatherBench/blob/master/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once only and dont commit\n",
    "#!git clone https://github.com/pangeo-data/WeatherBench/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'WeatherBench')\n",
    "from WeatherBench.src.train_nn import PeriodicConv2D, create_predictions#DataGenerator, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v='t2m'\n",
    "bs=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://s2s-ai-challenge.github.io/\n",
    "\n",
    "We deal with two fundamentally different variables here: \n",
    "- Total precipitation is precipitation flux pr accumulated over lead_time until valid_time and therefore describes a point observation. \n",
    "- 2m temperature is averaged over lead_time(valid_time) and therefore describes an average observation. \n",
    "\n",
    "The submission file data model unifies both approaches and assigns 14 days for week 3-4 and 28 days for week 5-6 marking the first day of the biweekly aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 bi-weekly `lead_time`: week 3-4\n",
    "lead_input = hind_2000_2019.lead_time[0]\n",
    "lead_output = obs_2000_2019.lead_time[0]\n",
    "#lead.values\n",
    "#lead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask: same missing values at all forecast_times\n",
    "mask = xr.where(obs_2000_2019.notnull(),1,np.nan).mean('forecast_time', skipna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "fct_train = hind_2000_2019.sel(forecast_time=slice(time_train_start,time_train_end))\n",
    "verif_train = obs_2000_2019_terciled.sel(forecast_time=slice(time_train_start,time_train_end))[v]\n",
    "\n",
    "verif_train = verif_train.where(mask[v].notnull())\n",
    "\n",
    "fct_train_mean = fct_train.mean('forecast_time')\n",
    "fct_train_std = fct_train.std('forecast_time')\n",
    "\n",
    "verif_train_mean = verif_train.mean('forecast_time')\n",
    "verif_train_std = verif_train.std('forecast_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "fct_valid = hind_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))#[v]\n",
    "verif_valid = obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "\n",
    "verif_valid = verif_valid.where(mask[v].notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove annual cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_annualcycle(ds, ds_train):\n",
    "    #remove annual cycle for each location \n",
    "    \n",
    "    ds = add_year_week_coords(ds)\n",
    "    ds_train = add_year_week_coords(ds_train)\n",
    "    \n",
    "    if 'realization' in ds_train.coords:#always use train data to compute the annual cycle\n",
    "        ens_mean = ds_train.mean('realization')\n",
    "    else:\n",
    "        ens_mean = ds_train\n",
    "\n",
    "    ds_stand = ds - ens_mean.groupby('week').mean(['forecast_time'])\n",
    "\n",
    "    ds_stand = ds_stand.sel({'week' : ds.coords['week']})\n",
    "    ds_stand = ds_stand.drop(['week','year'])\n",
    "    ds_stand\n",
    "    return ds_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###remove annual cycle here\n",
    "fct_train = rm_annualcycle(fct_train, fct_train)\n",
    "fct_valid = rm_annualcycle(fct_valid, fct_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use ensemble mean as input\n",
    "fct_train = fct_train.mean('realization')\n",
    "fct_valid = fct_valid.mean('realization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct_train.sel(lead_time = lead_input).isel(forecast_time = 0)[v].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_train.sel(lead_time = lead_output).isel(forecast_time = 0).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basis(out_field, r_basis):\n",
    "    \"\"\"returns a set of basis functions for the input field, adapted from Scheuerer et al. 2020.\n",
    "\n",
    "    PARAMETERS:\n",
    "    out_field : (xarray DataArray) basis functions for these lat lon coords will be created\n",
    "    r_basis : (int) radius of support of basis functions, \n",
    "                    the distance between centers of basis functions is half this radius,\n",
    "                    should be choosen depending on input field size.\n",
    "    \n",
    "    RETURNS:\n",
    "    basis : \n",
    "    lats : lats of input field\n",
    "    lons : lons of input field\n",
    "    n_xy : number of grid points in input field\n",
    "    n_basis : number of basis functions\n",
    "    \"\"\"  \n",
    "    \n",
    "    #r_basis = 14 #radius of support of basis functions\n",
    "    dist_basis = r_basis/2 #distance between centers of basis functions\n",
    "\n",
    "    lats = out_field.latitude\n",
    "    lons = out_field.longitude\n",
    "    \n",
    "    #number of basis functions\n",
    "    n_basis = int(np.ceil((lats[0] - lats[-1])/dist_basis + 1)*np.ceil((lons[-1] - lons[0])/dist_basis + 1))\n",
    "\n",
    "    #grid coords\n",
    "    lon_np = lons\n",
    "    lat_np = lats\n",
    "    \n",
    "    length_lon = len(lon_np)\n",
    "    length_lat = len(lat_np)\n",
    "    \n",
    "    lon_np = np.outer(lon_np, np.ones(length_lat)).reshape(int(length_lon * length_lat))\n",
    "    lat_np = np.outer(lat_np, np.ones(length_lon)).reshape(int(length_lon * length_lat))\n",
    "\n",
    "    #number of grid points\n",
    "    n_xy = int(length_lon*length_lat)\n",
    "\n",
    "    #centers of basis functions\n",
    "    lon_ctr = np.arange(lons[0],lons[-1] + dist_basis,dist_basis)\n",
    "    length_lon_ctr = len(lon_ctr) #number of center points in lon direction\n",
    "\n",
    "    lat_ctr = np.arange(lats[0],lats[-1] - dist_basis,- dist_basis)\n",
    "    length_lat_ctr = len(lat_ctr) #number of center points in lat direction\n",
    "\n",
    "    lon_ctr = np.outer(lon_ctr, np.ones(length_lat_ctr)).reshape(int(n_basis))\n",
    "    lat_ctr = np.outer(np.ones(length_lon_ctr), lat_ctr).reshape(int(n_basis))\n",
    "\n",
    "    #compute distances between fct grid and basis function centers\n",
    "    dst_lon = np.abs(np.subtract.outer(lon_np,lon_ctr).reshape(len(lons),len(lats),n_basis))#10,14\n",
    "    dst_lon = np.swapaxes(dst_lon, 0, 1)\n",
    "    dst_lat = np.abs(np.subtract.outer(lat_np,lat_ctr).reshape(len(lats),len(lons),n_basis))#'14,10'\n",
    "\n",
    "    dst = np.sqrt(dst_lon**2+dst_lat**2)\n",
    "    dst = np.swapaxes(dst, 0, 1).reshape(n_xy,n_basis)\n",
    "\n",
    "    #define basis functions\n",
    "    basis = np.where(dst>r_basis,0.,(1.-(dst/r_basis)**3)**3)#main step, zero outside, \n",
    "    basis = basis/np.sum(basis,axis=1)[:,None]#normalization at each grid point\n",
    "    nbs = basis.shape[1]\n",
    "    \n",
    "    return basis, lats, lons, n_xy, n_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis, lats, lons, n_xy, n_basis = get_basis(obs_2000_2019_terciled[v], 14) #30\n",
    "##the smaller you choose the radius of the basis functions, the more memory needs to be allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Reshape, Dot, Add, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameters for CNN\n",
    "\n",
    "n_bins = 3 #we have to predict probs for 3 bins\n",
    "dropout_rate = 0.4\n",
    "hidden_nodes = [10] #they tried different architectures\n",
    "clim_probs = np.log(1/3)*np.ones((n_xy,n_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN: slightly adapted from Scheuerer et al 2020.\n",
    "\n",
    "inp_imgs = Input(shape=(121,240,4,)) #fcts\n",
    "inp_basis = Input(shape=(n_xy,n_basis)) #basis\n",
    "inp_cl = Input(shape=(n_xy,n_bins,)) #climatology\n",
    "\n",
    "c = Conv2D(4, (3,3), activation='elu')(inp_imgs)\n",
    "c = MaxPooling2D((2,2))(c)\n",
    "c = Conv2D(8, (3,3), activation='elu')(c)\n",
    "c = MaxPooling2D((2,2))(c)\n",
    "x = Flatten()(c)\n",
    "for h in hidden_nodes: \n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(h, activation='elu')(x)\n",
    "x = Dense(n_bins*n_basis, activation='elu')(x)\n",
    "x = Reshape((n_bins,n_basis))(x)\n",
    "z = Dot(axes=2)([inp_basis, x])     # Tensor product with basis functions\n",
    "z = Add()([z, inp_cl])              # Add (log) probability anomalies to log climatological probabilities \n",
    "out = Activation('softmax')(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Model(inputs=[inp_imgs, inp_basis, inp_cl], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=keras.optimizers.Adam(1e-4))#'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow import set_random_seed\n",
    "from tensorflow import random\n",
    "random.set_seed(1)#this seems not to have any effect so far..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a data generator, ow it takes too long to convert to numpy. its also probably too large for an efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, fct, verif, lead_input, lead_output, basis, clim_probs, batch_size=32, shuffle=True, load=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "        Args:\n",
    "            fct: forecasts from S2S models: xr.DataArray (xr.Dataset doesnt work properly)\n",
    "            verif: not true#observations with same dimensionality (xr.Dataset doesnt work properly)\n",
    "            lead_time: Lead_time as in model\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "            \n",
    "        Todo:\n",
    "        - use number in a better way, now uses only ensemble mean forecast\n",
    "        - dont use .sel(lead_time=lead_time) to train over all lead_time at once\n",
    "        - be sensitive with forecast_time, pool a few around the weekofyear given\n",
    "        - use more variables as predictors\n",
    "        - predict more variables\n",
    "        \"\"\"\n",
    "        \n",
    "                \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_input = lead_input\n",
    "        self.lead_output = lead_output\n",
    "        \n",
    "        ###remove annual cycle here\n",
    "        ### add more variables\n",
    "        \n",
    "        self.fct_data = fct.transpose('forecast_time', ...).sel(lead_time=lead_input)\n",
    "        #self.fct_mean = self.fct_data.mean('forecast_time').compute() if mean_fct is None else mean_fct.sel(lead_time = lead)\n",
    "        #self.fct_std = self.fct_data.std('forecast_time').compute() if std_fct is None else std_fct.sel(lead_time = lead)\n",
    "        \n",
    "        self.verif_data = verif.transpose('forecast_time', ...).sel(lead_time=lead_output)\n",
    "        #self.verif_mean = self.verif_data.mean('forecast_time').compute() if mean_verif is None else mean_verif.sel(lead_time = lead)\n",
    "        #self.verif_std = self.verif_data.std('forecast_time').compute() if std_verif is None else std_verif.sel(lead_time = lead)\n",
    "\n",
    "        # Normalize\n",
    "        #self.fct_data = (self.fct_data - self.fct_mean) / self.fct_std\n",
    "        #self.verif_data = (self.verif_data - self.verif_mean) / self.verif_std\n",
    "    \n",
    "        \n",
    "        self.n_samples = self.fct_data.forecast_time.size\n",
    "        #self.forecast_time = self.fct_data.forecast_time\n",
    "        #self.n_lats = self.fct_data.latitude.size - self.window_size\n",
    "        #self.n_lons = self.fct_data.longitude.size - self.window_size\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load:\n",
    "            # print('Loading data into RAM')\n",
    "            self.fct_data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        #lats = self.lats [i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        #lons = self.lons[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        ##data comes in a row, not randomly chosen from within train data, if shuffled beforehand,--> stays shuffled because of isel\n",
    "        # got all nan if nans not masked\n",
    "        X_x = self.fct_data.isel(forecast_time=idxs).fillna(0.).to_array().transpose('forecast_time', ...,'variable').values#.values\n",
    "        \n",
    "        \n",
    "        X_basis = np.repeat(basis[np.newaxis,:,:],len(idxs),axis=0)\n",
    "        X_clim =  np.repeat(clim_probs[np.newaxis,:,:],len(idxs),axis=0)#self.batch_size\n",
    "        \n",
    "        X = [X_x, X_basis, X_clim]\n",
    "        \n",
    "        #X = self.fct_data.isel(forecast_time=idxs).isel(latitude = slice(lats,lats + self.window_size), \n",
    "         #                                               longitude = slice(lons,lons + self.window_size)).fillna(0.).values\n",
    "        #x_coords = (math.ceil((lats + self.window_size)/2),math.ceil((lons + self.window_size)/2))\n",
    "        y = self.verif_data.stack(Z = ['latitude','longitude']).transpose('forecast_time','Z',...).isel(forecast_time=idxs).fillna(0.).values\n",
    "        #y = self.verif_data.isel(forecast_time=idxs).fillna(0.).values\n",
    "        #y = self.verif_data.isel(forecast_time=idxs).isel(latitude = x_coords[0], \n",
    "         #                                                 longitude = x_coords[1]).fillna(0.).values\n",
    "        \n",
    "        return X, y # x_coords,\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        #self.lats = np.arange(self.n_lats)\n",
    "        #self.lons = np.arange(self.n_lons)\n",
    "        if self.shuffle == True: ###does this make sense here?\n",
    "            np.random.shuffle(self.idxs)\n",
    "            #np.random.shuffle(self.lats)\n",
    "            #np.random.shuffle(self.lons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the data generators takes too much time, probably because everything has to be opened because of rm_annualcycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train = DataGenerator(fct_train, verif_train,#.sel(forecast_time = slice('2000','2001'))\n",
    "                         lead_input=lead_input,lead_output = lead_output,  basis = basis, clim_probs = clim_probs,\n",
    "                         batch_size=bs, load=True)#,\n",
    "                         #mean_fct=fct_train_mean, std_fct=fct_train_std, \n",
    "                         #mean_verif = verif_train_mean, std_verif = verif_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train[-1][0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_valid = DataGenerator(fct_valid, verif_valid,\n",
    "                         lead_input=lead_input,lead_output = lead_output, basis = basis, clim_probs = clim_probs, batch_size=bs, load=True)#,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_valid[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_valid[-1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_valid[-1][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_valid[-1][0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(dg_train, \n",
    "         epochs=5, shuffle = True,\n",
    "        validation_data = dg_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some ideas for improvements:\n",
    "#circular input\n",
    "#https://www.tu-chemnitz.de/etit/proaut/publications/schubert19_IV.pdf\n",
    "#https://www.tu-chemnitz.de/etit/proaut/en/research/ccnn.html\n",
    "#add indices\n",
    "#https://stackoverflow.com/questions/47818968/adding-an-additional-value-to-a-convolutional-neural-network-input\n",
    "#https://datascience.stackexchange.com/questions/68450/how-can-you-include-information-not-present-in-an-image-for-neural-networks\n",
    "#https://www.nature.com/articles/s41598-019-42294-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define create_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_predictions(model, dg, lead):\n",
    "    \"\"\"Create non-iterative predictions\n",
    "    returns: prediction in the shape of the input arguments to DataGenerator classe\n",
    "    \"\"\"\n",
    "    import tensorflow as tf \n",
    "    \n",
    "    preds = model.predict(dg).squeeze()\n",
    "    \n",
    "    preds = Reshape((len(lons),len(lats),3))(preds)\n",
    "    preds = tf.transpose(preds, [0,3,1,2])\n",
    "\n",
    "    da = xr.DataArray(\n",
    "                preds,\n",
    "                dims=['forecast_time', 'category','longitude', 'latitude'],\n",
    "                coords={'forecast_time': fct_valid.forecast_time, 'category' : verif_train.category, 'latitude': verif_train.latitude,\n",
    "                        'longitude': verif_train.longitude}\n",
    "            )\n",
    "    da = da.transpose('forecast_time','category','latitude',...)\n",
    "    da = da.assign_coords(lead_time=lead_output)\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_valid_time_single(forecast, init_dim='forecast_time'):\n",
    "    \"\"\"Creates valid_time(forecast_time, lead_time) for a single lead time and variable\n",
    "    \n",
    "    lead_time: pd.Timedelta\n",
    "    forecast_time: datetime\n",
    "    \"\"\"\n",
    "    times = xr.DataArray(\n",
    "                forecast[init_dim] + lead_output,\n",
    "                dims=init_dim,\n",
    "                coords={init_dim: forecast[init_dim]},\n",
    "            )\n",
    "            \n",
    "    forecast = forecast.assign_coords(valid_time=times)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_prediction(cnn, dg, lead):\n",
    "    \"\"\"prediction for one var and one lead-time\n",
    "    \n",
    "    args:\n",
    "    time: time slice\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    preds_test = _create_predictions(cnn, dg, lead)\n",
    "    \n",
    "    # add valid_time coord\n",
    "    ###preds_test = add_valid_time_from_forecast_reference_time_and_lead_time(preds_test)\n",
    "    # only works for complete output\n",
    "    preds_test = add_valid_time_single(preds_test)\n",
    "    #preds_test = preds_test.to_dataset(name=v)\n",
    "                                    \n",
    "    return preds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction from CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cnn.predict(dg_valid).squeeze()\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import tensorflow as tf\n",
    "    preds = Reshape((len(lons),len(lats),3))(preds)\n",
    "    preds = tf.transpose(preds, [0,3,1,2])\n",
    "    preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_single = single_prediction(cnn, \n",
    "                                 [fct_valid.sel(lead_time = lead_input).fillna(0.).to_array().transpose('forecast_time', ...,'variable').values,\n",
    "                                   np.repeat(basis[np.newaxis,:,:],len(fct_valid.forecast_time),axis=0),\n",
    "                                 np.repeat(clim_probs[np.newaxis,:,:],len(fct_valid.forecast_time),axis=0)],\n",
    "                                  lead_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_masked = preds_single.where(mask[v].sel(lead_time = lead_output).notnull())\n",
    "preds_masked.isel(forecast_time = 0).plot(col = 'category')#, vmin = 0, vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_valid.sel(lead_time = lead_output).isel(forecast_time = 0).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_valid.sel(lead_time = lead_output).isel(forecast_time = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tercile probs of the raw ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "#tercile_edges : used in create_predictions --> make_probabilistic\n",
    "#mask: used in make_probabilistic, but make_probabilistic would also work without mask\n",
    "\n",
    "#!git lfs pull ../template/data/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc\n",
    "#tercile_file = f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc'\n",
    "#tercile_edges = xr.open_dataset(tercile_file)\n",
    "tercile_edges = load_data(data = 'obs_tercile_edges_2000-2019', aggregation = 'biweekly', path = path_data)\n",
    "#obs_2000_2019_terciled = load_data(data = 'obs_terciled_2000-2019', aggregation = 'biweekly', path = path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = hind_2000_2019.sel(forecast_time=slice(time_valid_start,time_valid_end))[v]\n",
    "test_raw = make_probabilistic(test, tercile_edges)\n",
    "test_raw.isel(lead_time = 0).isel(forecast_time = 0)['t2m'].where(mask[v].sel(lead_time = lead_output).notnull()).plot(col = 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check: probs of terciles add up to one\n",
    "#preds_single.isel(forecast_time = 0).sum('category').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute RPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes RPSS wrt climatology (1/3 for each category. So, negative RPSS are worse than climatology...\n",
    "\n",
    "def skill_by_year_single(prediction, terciled_obs):\n",
    "    \"\"\"version of skill_by_year adjusted to one var and one lead time and flexibel validation period\"\"\"\n",
    "    fct_p = prediction\n",
    "    obs_p = terciled_obs\n",
    "\n",
    "\n",
    "    # climatology\n",
    "    clim_p = xr.DataArray([1/3, 1/3, 1/3], dims='category', coords={'category':['below normal', 'near normal', 'above normal']}).to_dataset(name='tp')\n",
    "    clim_p['t2m'] = clim_p['tp']\n",
    "\n",
    "    clim_p = clim_p[v]\n",
    "\n",
    "    ## RPSS\n",
    "    # rps_ML\n",
    "    rps_ML = xs.rps(obs_p, fct_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    # rps_clim\n",
    "    rps_clim = xs.rps(obs_p, clim_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "\n",
    "    # rpss\n",
    "    rpss = 1 - (rps_ML / rps_clim)\n",
    "\n",
    "    # https://renkulab.io/gitlab/aaron.spring/s2s-ai-challenge-template/-/issues/7\n",
    "\n",
    "    # penalize\n",
    "    penalize = obs_p.where(fct_p!=1, other=-10).mean('category')\n",
    "    rpss = rpss.where(penalize!=0, other=-10)\n",
    "\n",
    "    # clip\n",
    "    rpss = rpss.clip(-10, 1)\n",
    "\n",
    "    # average over all forecasts\n",
    "    rpss_year = rpss.groupby('forecast_time.year').mean()\n",
    "\n",
    "    # weighted area mean\n",
    "    weights = np.cos(np.deg2rad(np.abs(rpss_year.latitude)))\n",
    "    # spatially weighted score averaged over lead_times and variables to one single value\n",
    "    scores = rpss_year.sel(latitude=slice(None, -60)).weighted(weights).mean('latitude').mean('longitude')\n",
    "    #scores = scores.to_array().mean(['lead_time', 'variable'])\n",
    "\n",
    "    return scores.to_dataframe('RPSS') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].sel(lead_time = lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_by_year_single(preds_single, \n",
    "                     obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].sel(lead_time = lead_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPSS in the order of -0.015 (for r_basis = 14)\n",
    "# if you choose a larger radius for the basis functions, we might come closer to climatology and hence achieve a better RPSS.\n",
    "# fluctuations in predictions are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_by_year_single(test_raw[v].sel(latitude = output_lat, longitude = output_lon).sel(lead_time = lead_output),\n",
    "                     obs_2000_2019_terciled.sel(forecast_time=slice(time_valid_start,time_valid_end))[v].sel(lead_time = lead_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPSS in the order of -0.635"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The RPSS of this CNN approach is higher than for all ANNs. \n",
    "CNN fields are less smooth than the fields of ANN terciled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://phoenixnap.com/kb/linux-commands-check-memory-usage\n",
    "!free -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
