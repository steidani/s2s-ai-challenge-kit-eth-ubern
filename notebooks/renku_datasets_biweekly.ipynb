{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create biweekly renku datasets from `climetlab-s2s-ai-challenge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "- Create biweekly renku datasets from [`climatelab-s2s-ai-challenge`](https://github.com/ecmwf-lab/climetlab-s2s-ai-challenge).\n",
    "- These renku datasets are then used in notebooks:\n",
    "    - `ML_train_and_predict.ipynb` to train the ML model and do ML-based predictions\n",
    "    - `RPSS_verification.ipynb` to calculate RPSS of the ML model\n",
    "\n",
    "Requirements:\n",
    "- [`climetlab`](https://github.com/ecmwf/climetlab)\n",
    "- [`climatelab-s2s-ai-challenge`](https://github.com/ecmwf-lab/climetlab-s2s-ai-challenge)\n",
    "- S2S and CPC observations uploaded on [European Weather Cloud (EWC)](https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/training-input/0.3.0/netcdf/index.html)\n",
    "\n",
    "Output: [renku dataset](https://renku-python.readthedocs.io/en/latest/commands.html#module-renku.cli.dataset) `s2s-ai-challenge`\n",
    "- observations\n",
    "    - deterministic:\n",
    "        - `hindcast-like-observations_2000-2019_biweekly_deterministic.zarr`\n",
    "        - `forecast-like-observations_2020_biweekly_deterministic.zarr`\n",
    "    - edges:\n",
    "        - `hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc`\n",
    "    - probabilistic:\n",
    "        - `hindcast-like-observations_2000-2019_biweekly_terciled.zarr`\n",
    "        - `forecast-like-observations_2020_biweekly_terciled.nc`\n",
    "- forecasts/hindcasts\n",
    "    - deterministic:\n",
    "        - `ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr`\n",
    "        - `ecmwf_forecast-input_2020_biweekly_deterministic.zarr`\n",
    "    - more models could be added\n",
    "- benchmark:\n",
    "    - probabilistic:\n",
    "        - `ecmwf_recalibrated_benchmark_2020_biweekly_terciled.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climetlab version : 0.8.0\n",
      "Climetlab-s2s-ai-challenge plugin version : 0.6.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: ecmwflibs universal: cannot find a library called MagPlus\n",
      "/work/mh0727/m300524/conda-envs/s2s-ai/lib/python3.7/site-packages/climetlab/plotting/drivers/magics/actions.py:36: UserWarning: Magics library could not be found\n",
      "  warnings.warn(str(e))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x2b4cbd09de10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "import pandas as pd\n",
    "\n",
    "import climetlab_s2s_ai_challenge\n",
    "import climetlab as cml\n",
    "print(f'Climetlab version : {cml.__version__}')\n",
    "print(f'Climetlab-s2s-ai-challenge plugin version : {climetlab_s2s_ai_challenge.__version__}')\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "xr.set_options(display_style='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching path for climetlab\n",
    "cache_path = \"/work/mh0727/m300524/S2S_AI/cache\" # set your own path\n",
    "cml.settings.set(\"cache-directory\", cache_path)\n",
    "\n",
    "cache_path = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and cache\n",
    "\n",
    "Download all files for the observations, forecast and hindcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut\n",
    "from scripts import download\n",
    "#download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hindcast and forecast `input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting dates forecast_time in 2020\n",
    "dates = xr.cftime_range(start='20200102',freq='7D', periods=53).strftime('%Y%m%d').to_list()\n",
    "\n",
    "forecast_dataset_labels = ['training-input','test-input'] # ML community\n",
    "# equiv to\n",
    "forecast_dataset_labels = ['hindcast-input','forecast-input'] # NWP community\n",
    "\n",
    "varlist_forecast = ['tp','t2m'] # can add more\n",
    "\n",
    "center_list = ['ecmwf'] # 'ncep', 'eccc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes ~ 10-30 min to download for one model one variable depending on number of model realizations\n",
    "# and download settings https://climetlab.readthedocs.io/en/latest/guide/settings.html \n",
    "for center in center_list:\n",
    "    for ds in forecast_dataset_labels:\n",
    "        cml.load_dataset(f\"s2s-ai-challenge-{ds}\", origin=center, parameter=varlist_forecast, format='netcdf').to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## observations `output-reference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dataset_labels = ['training-output-reference','test-output-reference'] # ML community\n",
    "# equiv to\n",
    "obs_dataset_labels = ['hindcast-like-observations','forecast-like-observations'] # NWP community\n",
    "\n",
    "varlist_obs = ['tp', 't2m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes 10min to download\n",
    "for ds in obs_dataset_labels:\n",
    "    print(ds)\n",
    "    # only netcdf, no format choice\n",
    "    cml.load_dataset(f\"s2s-ai-challenge-{ds}\", date=dates, parameter=varlist_obs).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download obs_time for to create output-reference/observations for other models than ecmwf and eccc,\n",
    "# i.e. ncep or any S2S or Sub model\n",
    "obs_time = cml.load_dataset(f\"s2s-ai-challenge-observations\", parameter=['t2m', 'pr']).to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create bi-weekly aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import aggregate_biweekly, ensure_attributes\n",
    "\n",
    "#aggregate_biweekly??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, center in enumerate(center_list):  # forecast centers (could also take models)\n",
    "    for dsl in obs_dataset_labels + forecast_dataset_labels:  # climetlab dataset labels\n",
    "        for p, parameter in enumerate(varlist_forecast):  # variables\n",
    "            if c != 0 and 'observation' in dsl:  # only do once for observations \n",
    "                continue\n",
    "            print(f\"datasetlabel: {dsl}, center: {center}, parameter: {parameter}\")\n",
    "            if 'input' in dsl:\n",
    "                ds = cml.load_dataset(f\"s2s-ai-challenge-{dsl}\", origin=center, parameter=parameter, format='netcdf').to_xarray()\n",
    "            elif 'observation' in dsl: # obs only netcdf, no choice\n",
    "                if parameter not in ['t2m', 'tp']:\n",
    "                    continue\n",
    "                ds = cml.load_dataset(f\"s2s-ai-challenge-{dsl}\", parameter=parameter, date=dates).to_xarray()\n",
    "\n",
    "            if p == 0:\n",
    "                ds_biweekly = ds.map(aggregate_biweekly)\n",
    "            else:\n",
    "                ds_biweekly[parameter] = ds.map(aggregate_biweekly)[parameter]\n",
    "\n",
    "            ds_biweekly = ds_biweekly.map(ensure_attributes, biweekly=True)\n",
    "\n",
    "        if 'test' in dsl:\n",
    "            ds_biweekly = ds_biweekly.chunk('auto')\n",
    "        else:\n",
    "            ds_biweekly = ds_biweekly.chunk({'forecast_time':'auto','lead_time':-1,'longitude':-1,'latitude':-1})\n",
    "\n",
    "        if 'hindcast' in dsl:\n",
    "            time = f'{int(ds_biweekly.forecast_time.dt.year.min())}-{int(ds_biweekly.forecast_time.dt.year.max())}'\n",
    "            if 'input' in dsl:\n",
    "                name = f'{center}_{dsl}'\n",
    "            elif 'observations':\n",
    "                name = dsl\n",
    "\n",
    "        elif 'forecast' in dsl:\n",
    "            time = '2020'\n",
    "            if 'input' in dsl:\n",
    "                name = f'{center}_{dsl}'\n",
    "            elif 'observations':\n",
    "                name = dsl\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        # pattern: {model_if_not_observations}{observations/forecast/hindcast}_{time}_biweekly_deterministic.zarr\n",
    "        zp = f'{cache_path}/{name}_{time}_biweekly_deterministic.zarr'\n",
    "        ds_biweekly.attrs.update({'postprocessed':'by https://renkulab.io/gitlab/aaron.spring/s2s-ai-challenge-template/-/blob/master/notebooks/renku_datasets_biweekly.ipynb'})\n",
    "        print(f'save to: {zp}')\n",
    "        ds_biweekly.astype('float32').to_zarr(zp, consolidated=True, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add to `renku` dataset `s2s-ai-challenge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations as hindcast\n",
    "# run renku commands from projects root directory only\n",
    "# !renku dataset add s2s-ai-challenge data/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for further use retrieve from git lfs\n",
    "# !renku storage pull ../data/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen(SortedKeysDict({'forecast_time': 1060, 'latitude': 121, 'lead_time': 2, 'longitude': 240})) \n",
      " Coordinates:\n",
      "  * forecast_time  (forecast_time) datetime64[ns] 2000-01-02 ... 2019-12-31\n",
      "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
      "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
      "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
      "    valid_time     (lead_time, forecast_time) datetime64[ns] dask.array<chunksize=(2, 1060), meta=np.ndarray> \n",
      " 492.546744 MB\n"
     ]
    }
   ],
   "source": [
    "obs_2000_2019 = xr.open_zarr(f\"{cache_path}/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr\", consolidated=True)\n",
    "print(obs_2000_2019.sizes,'\\n',obs_2000_2019.coords,'\\n', obs_2000_2019.nbytes/1e6,'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations as forecast\n",
    "# run renku commands from projects root directory only\n",
    "# !renku dataset add s2s-ai-challenge data/forecast-like-observations_2020_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen(SortedKeysDict({'forecast_time': 53, 'latitude': 121, 'lead_time': 2, 'longitude': 240})) \n",
      " Coordinates:\n",
      "  * forecast_time  (forecast_time) datetime64[ns] 2020-01-02 ... 2020-12-31\n",
      "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
      "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
      "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
      "    valid_time     (lead_time, forecast_time) datetime64[ns] dask.array<chunksize=(2, 53), meta=np.ndarray> \n",
      " 24.630096 MB\n"
     ]
    }
   ],
   "source": [
    "obs_2020 = xr.open_zarr(f\"{cache_path}/forecast-like-observations_2020_biweekly_deterministic.zarr\", consolidated=True)\n",
    "print(obs_2020.sizes,'\\n',obs_2020.coords,'\\n', obs_2020.nbytes/1e6,'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecmwf hindcast-input\n",
    "# run renku commands from projects root directory only\n",
    "# !renku dataset add s2s-ai-challenge data/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen(SortedKeysDict({'forecast_time': 1060, 'latitude': 121, 'lead_time': 2, 'longitude': 240, 'realization': 11})) \n",
      " Coordinates:\n",
      "  * forecast_time  (forecast_time) datetime64[ns] 2000-01-02 ... 2019-12-31\n",
      "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
      "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
      "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
      "  * realization    (realization) int64 0 1 2 3 4 5 6 7 8 9 10\n",
      "    valid_time     (lead_time, forecast_time) datetime64[ns] dask.array<chunksize=(2, 1060), meta=np.ndarray> \n",
      " 5417.730832 MB\n"
     ]
    }
   ],
   "source": [
    "hind_2000_2019 = xr.open_zarr(f\"{cache_path}/ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr\", consolidated=True)\n",
    "print(hind_2000_2019.sizes,'\\n',hind_2000_2019.coords,'\\n', hind_2000_2019.nbytes/1e6,'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecmwf forecast-input\n",
    "# run renku commands from projects root directory only\n",
    "# !renku dataset add s2s-ai-challenge data/ecmwf_forecast-input_2020_biweekly_deterministic.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen(SortedKeysDict({'forecast_time': 53, 'latitude': 121, 'lead_time': 2, 'longitude': 240, 'realization': 51})) \n",
      " Coordinates:\n",
      "  * forecast_time  (forecast_time) datetime64[ns] 2020-01-02 ... 2020-12-31\n",
      "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
      "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
      "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
      "  * realization    (realization) int64 0 1 2 3 4 5 6 7 ... 44 45 46 47 48 49 50\n",
      "    valid_time     (lead_time, forecast_time) datetime64[ns] dask.array<chunksize=(2, 53), meta=np.ndarray> \n",
      " 1255.926504 MB\n"
     ]
    }
   ],
   "source": [
    "fct_2020 = xr.open_zarr(f\"{cache_path}/ecmwf_forecast-input_2020_biweekly_deterministic.zarr\", consolidated=True)\n",
    "print(fct_2020.sizes,'\\n',fct_2020.coords,'\\n', fct_2020.nbytes/1e6,'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tercile edges\n",
    "\n",
    "Create 2 tercile edges at 1/3 and 2/3 quantiles of the 2000-2019 biweekly distrbution for each week of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tercile_file = f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mh0727/m300524/conda-envs/s2s-ai/lib/python3.7/site-packages/xarray/core/accessor_dt.py:381: FutureWarning: dt.weekofyear and dt.week have been deprecated. Please use dt.isocalendar().week instead.\n",
      "  FutureWarning,\n",
      "/work/mh0727/m300524/conda-envs/s2s-ai/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1390: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 25s, sys: 9min 19s, total: 30min 45s\n",
      "Wall time: 18min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr',\n",
    "             consolidated=True).chunk({'forecast_time':-1,'longitude':'auto'}).groupby('forecast_time.weekofyear').quantile(q=[1./3.,2./3.], dim=['forecast_time']).rename({'quantile':'category_edge'}).astype('float32').to_netcdf(tercile_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:        (category_edge: 2, latitude: 121, lead_time: 2, longitude: 240, weekofyear: 53)\n",
       "Coordinates:\n",
       "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
       "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
       "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
       "  * category_edge  (category_edge) float64 0.3333 0.6667\n",
       "  * weekofyear     (weekofyear) int64 1 2 3 4 5 6 7 8 ... 47 48 49 50 51 52 53\n",
       "Data variables:\n",
       "    t2m            (weekofyear, category_edge, lead_time, latitude, longitude) float32 ...\n",
       "    tp             (weekofyear, category_edge, lead_time, latitude, longitude) float32 ...\n",
       "Attributes:\n",
       "    created_by_script:    tools/observations/makefile\n",
       "    created_by_software:  climetlab-s2s-ai-challenge\n",
       "    function:             climetlab_s2s_ai_challenge.extra.forecast_like_obse...\n",
       "    postprocessed:        by https://renkulab.io/gitlab/aaron.spring/s2s-ai-c...\n",
       "    source_dataset_name:  NOAA NCEP CPC UNIFIED_PRCP GAUGE_BASED GLOBAL v1p0 ...\n",
       "    source_hosting:       IRIDL\n",
       "    source_url:           http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP/...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (category_edge: 2, latitude: 121, lead_time: 2, longitude: 240, weekofyear: 53)\n",
       "Coordinates:\n",
       "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
       "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
       "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
       "  * category_edge  (category_edge) float64 0.3333 0.6667\n",
       "  * weekofyear     (weekofyear) int64 1 2 3 4 5 6 7 8 ... 47 48 49 50 51 52 53\n",
       "Data variables:\n",
       "    t2m            (weekofyear, category_edge, lead_time, latitude, longitude) float32 ...\n",
       "    tp             (weekofyear, category_edge, lead_time, latitude, longitude) float32 ...\n",
       "Attributes:\n",
       "    created_by_script:    tools/observations/makefile\n",
       "    created_by_software:  climetlab-s2s-ai-challenge\n",
       "    function:             climetlab_s2s_ai_challenge.extra.forecast_like_obse...\n",
       "    postprocessed:        by https://renkulab.io/gitlab/aaron.spring/s2s-ai-c...\n",
       "    source_dataset_name:  NOAA NCEP CPC UNIFIED_PRCP GAUGE_BASED GLOBAL v1p0 ...\n",
       "    source_hosting:       IRIDL\n",
       "    source_url:           http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP/..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tercile_edges = xr.open_dataset(tercile_file)\n",
    "\n",
    "tercile_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49.255184, 'MB')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tercile_edges.nbytes*1e-6,'MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run renku commands from projects root directory only\n",
    "# tercile edges\n",
    "#!renku dataset add s2s-ai-challenge data/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use retrieve from git lfs\n",
    "#!renku storage pull ../data/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc\n",
    "#xr.open_dataset(\"../data/hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# observations in categories\n",
    "\n",
    "- counting how many deterministic forecasts realizations fall into each category, like counting rps\n",
    "- categorize forecast-like-observations 2020 into categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frozen(SortedKeysDict({'forecast_time': 53, 'latitude': 121, 'lead_time': 2, 'longitude': 240}))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_2020 = xr.open_zarr(f'{cache_path}/forecast-like-observations_2020_biweekly_deterministic.zarr', consolidated=True)\n",
    "obs_2020.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask for land grid\n",
    "mask = obs_2020.std(['lead_time','forecast_time']).notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask.to_array().plot(col='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total precipitation in arid regions are masked\n",
    "# Frederic Vitart suggested by email: \"Based on your map we could mask all the areas where the lower tercile boundary is lower than 0.1 mm\"\n",
    "# we are using a dry mask as in https://doi.org/10.1175/MWR-D-17-0092.1\n",
    "th = 0.01\n",
    "tp_arid_mask = tercile_edges.tp.isel(category_edge=0, lead_time=0, drop=True) > th\n",
    "#tp_arid_mask.where(mask.tp).plot(col='forecast_time', col_wrap=4)\n",
    "#plt.suptitle(f'dry mask: week 3-4 tp 1/3 category_edge > {th} kg m-2',y=1., x=.4)\n",
    "#plt.savefig('dry_mask.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into tercile edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tercile_edges.isel(forecast_time=0)['tp'].plot(col='lead_time',row='category_edge', robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tercile_edges.isel(forecast_time=[0,20],category_edge=1)['tp'].plot(col='lead_time', row='forecast_time', robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tercile_edges.tp.mean(['forecast_time']).plot(col='lead_time',row='category_edge',vmax=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorize observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forecast 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import make_probabilistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mh0727/m300524/conda-envs/s2s-ai/lib/python3.7/site-packages/xarray/core/accessor_dt.py:381: FutureWarning: dt.weekofyear and dt.week have been deprecated. Please use dt.isocalendar().week instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "obs_2020_p = make_probabilistic(obs_2020, tercile_edges, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147.75984, 'MB')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_2020_p.nbytes/1e6, 'MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mh0727/m300524/conda-envs/s2s-ai/lib/python3.7/site-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = np.divide(x1, x2, out)\n"
     ]
    }
   ],
   "source": [
    "obs_2020_p.astype('float32').to_netcdf(f'{cache_path}/forecast-like-observations_2020_biweekly_terciled.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast-like-observations terciled\n",
    "# run renku commands from projects root directory only\n",
    "# !renku dataset add s2s-ai-challenge data/forecast-like-observations_2020_biweekly_terciled.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:        (category: 3, forecast_time: 53, latitude: 121, lead_time: 2, longitude: 240)\n",
       "Coordinates:\n",
       "  * forecast_time  (forecast_time) datetime64[ns] 2020-01-02 ... 2020-12-31\n",
       "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
       "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
       "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
       "    valid_time     (lead_time, forecast_time) datetime64[ns] ...\n",
       "  * category       (category) object &#x27;below normal&#x27; &#x27;near normal&#x27; &#x27;above normal&#x27;\n",
       "Data variables:\n",
       "    t2m            (category, lead_time, forecast_time, latitude, longitude) float32 ...\n",
       "    tp             (category, lead_time, forecast_time, latitude, longitude) float32 ...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (category: 3, forecast_time: 53, latitude: 121, lead_time: 2, longitude: 240)\n",
       "Coordinates:\n",
       "  * forecast_time  (forecast_time) datetime64[ns] 2020-01-02 ... 2020-12-31\n",
       "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
       "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
       "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
       "    valid_time     (lead_time, forecast_time) datetime64[ns] ...\n",
       "  * category       (category) object 'below normal' 'near normal' 'above normal'\n",
       "Data variables:\n",
       "    t2m            (category, lead_time, forecast_time, latitude, longitude) float32 ...\n",
       "    tp             (category, lead_time, forecast_time, latitude, longitude) float32 ..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use retrieve from git lfs\n",
    "#!renku storage pull ../data/forecast-like-observations_2020_biweekly_terciled.nc\n",
    "xr.open_dataset(\"../data/forecast-like-observations_2020_biweekly_terciled.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hindcast 2000_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2000_2019 = xr.open_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_deterministic.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mh0727/m300524/conda-envs/s2s-ai/lib/python3.7/site-packages/xarray/core/accessor_dt.py:381: FutureWarning: dt.weekofyear and dt.week have been deprecated. Please use dt.isocalendar().week instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "obs_2000_2019_p = make_probabilistic(obs_2000_2019, tercile_edges, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2955.138888, 'MB')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_2000_2019_p.nbytes/1e6, 'MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mh0727/m300524/conda-envs/s2s-ai/lib/python3.7/site-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = np.divide(x1, x2, out)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x2b34e40d80c0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_2000_2019_p.astype('float32').chunk('auto').to_zarr(f'{cache_path}/hindcast-like-observations_2000-2019_biweekly_terciled.zarr', consolidated=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast-like-observations terciled\n",
    "# run renku commands from projects root directory only\n",
    "# !renku dataset add s2s-ai-challenge data/hindcast-like-observations_2000-2019_biweekly_terciled.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:        (category: 3, forecast_time: 1060, latitude: 121, lead_time: 2, longitude: 240)\n",
       "Coordinates:\n",
       "  * category       (category) &lt;U12 &#x27;below normal&#x27; &#x27;near normal&#x27; &#x27;above normal&#x27;\n",
       "  * forecast_time  (forecast_time) datetime64[ns] 2000-01-02 ... 2019-12-31\n",
       "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
       "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
       "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
       "    valid_time     (lead_time, forecast_time) datetime64[ns] dask.array&lt;chunksize=(2, 1060), meta=np.ndarray&gt;\n",
       "Data variables:\n",
       "    t2m            (category, lead_time, forecast_time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 2, 280, 121, 240), meta=np.ndarray&gt;\n",
       "    tp             (category, lead_time, forecast_time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 2, 280, 121, 240), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (category: 3, forecast_time: 1060, latitude: 121, lead_time: 2, longitude: 240)\n",
       "Coordinates:\n",
       "  * category       (category) <U12 'below normal' 'near normal' 'above normal'\n",
       "  * forecast_time  (forecast_time) datetime64[ns] 2000-01-02 ... 2019-12-31\n",
       "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
       "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
       "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
       "    valid_time     (lead_time, forecast_time) datetime64[ns] dask.array<chunksize=(2, 1060), meta=np.ndarray>\n",
       "Data variables:\n",
       "    t2m            (category, lead_time, forecast_time, latitude, longitude) float32 dask.array<chunksize=(1, 2, 280, 121, 240), meta=np.ndarray>\n",
       "    tp             (category, lead_time, forecast_time, latitude, longitude) float32 dask.array<chunksize=(1, 2, 280, 121, 240), meta=np.ndarray>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use retrieve from git lfs\n",
    "#!renku storage pull ../data/hindcast-like-observations_2000-2019_biweekly_terciled.zarr\n",
    "xr.open_zarr(\"../data/hindcast-like-observations_2000-2019_biweekly_terciled.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "center: ECMWF\n",
    "\n",
    "The calibration has been performed by using the tercile boundaries from the model climatology rather than from observations. Script by Frederic Vitart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By downloading data from this dataset, you agree to the terms and conditions defined at https://apps.ecmwf.int/datasets/data/s2s/licence/. If you do not agree with such terms, do not download the data. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.89it/s]\n",
      "WARNING: ecmwflibs universal: found eccodes at /work/mh0727/m300524/conda-envs/s2s-ai/lib/libeccodes.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: ecCodes 2.21.0 or higher is recommended. You are running version 2.12.3\n"
     ]
    }
   ],
   "source": [
    "bench_p = cml.load_dataset(\"s2s-ai-challenge-test-output-benchmark\", parameter=['tp','t2m']).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_p['category'].attrs = {'long_name': 'tercile category probabilities', 'units': '1',\n",
    "                        'description': 'Probabilities for three tercile categories. All three tercile category probabilities must add up to 1.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_p['lead_time'] = [pd.Timedelta(f\"{i} d\") for i in [14, 28]] # take first day of biweekly average as new coordinate\n",
    "\n",
    "bench_p['lead_time'].attrs = {'long_name':'forecast_period', 'description': 'Forecast period is the time interval between the forecast reference time and the validity time.',\n",
    "                         'aggregate': 'The pd.Timedelta corresponds to the first day of a biweekly aggregate.',\n",
    "                         'week34_t2m': 'mean[day 14, 27]',\n",
    "                         'week56_t2m': 'mean[day 28, 41]',\n",
    "                         'week34_tp': 'day 28 minus day 14',\n",
    "                         'week56_tp': 'day 42 minus day 28'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_p = bench_p / 100 # convert percent to [0-1] probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By downloading data from this dataset, you agree to the terms and conditions defined at https://apps.ecmwf.int/datasets/data/s2s/licence/. If you do not agree with such terms, do not download the data. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "bench_p = bench_p.map(ensure_attributes, biweekly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bench_p.isel(forecast_time=2).t2m.plot(row='lead_time', col='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:        (category: 3, forecast_time: 53, latitude: 121, lead_time: 2, longitude: 240)\n",
       "Coordinates:\n",
       "  * category       (category) object &#x27;below normal&#x27; &#x27;near normal&#x27; &#x27;above normal&#x27;\n",
       "  * forecast_time  (forecast_time) datetime64[ns] 2020-01-02 ... 2020-12-31\n",
       "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
       "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
       "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
       "    valid_time     (forecast_time, lead_time) datetime64[ns] 2020-01-16 ... 2...\n",
       "Data variables:\n",
       "    tp             (category, forecast_time, lead_time, latitude, longitude) float32 ...\n",
       "    t2m            (category, forecast_time, lead_time, latitude, longitude) float32 ...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (category: 3, forecast_time: 53, latitude: 121, lead_time: 2, longitude: 240)\n",
       "Coordinates:\n",
       "  * category       (category) object 'below normal' 'near normal' 'above normal'\n",
       "  * forecast_time  (forecast_time) datetime64[ns] 2020-01-02 ... 2020-12-31\n",
       "  * lead_time      (lead_time) timedelta64[ns] 14 days 28 days\n",
       "  * latitude       (latitude) float64 90.0 88.5 87.0 85.5 ... -87.0 -88.5 -90.0\n",
       "  * longitude      (longitude) float64 0.0 1.5 3.0 4.5 ... 355.5 357.0 358.5\n",
       "    valid_time     (forecast_time, lead_time) datetime64[ns] 2020-01-16 ... 2...\n",
       "Data variables:\n",
       "    tp             (category, forecast_time, lead_time, latitude, longitude) float32 ...\n",
       "    t2m            (category, forecast_time, lead_time, latitude, longitude) float32 ..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_p.astype('float32').to_netcdf('../data/ecmwf_recalibrated_benchmark_2020_biweekly_terciled.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!renku dataset add s2s-ai-challenge data/ecmwf_recalibrated_benchmark_2020_biweekly_terciled.nc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
